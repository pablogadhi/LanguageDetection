{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import random\n",
    "import os\n",
    "import requests\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from os import listdir, remove, mkdir\n",
    "from os.path import join as join_path\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "from nltk.translate.meteor_score import meteor_score\n",
    "from pyonmttok import Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing europarl data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "euro_data_dir = \"../europarl/aligned/\"\n",
    "output_data_dir = \"./data/original/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Join Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for l_pair in listdir(euro_data_dir):\n",
    "    pair_path = join_path(euro_data_dir, l_pair)\n",
    "    for l_dir in listdir(pair_path):\n",
    "        out_file = open(output_data_dir + l_pair + \"_\" + l_dir + \".txt\", \"w\")\n",
    "        l_path = join_path(pair_path, l_dir)\n",
    "        for file_name in sorted(listdir(l_path)):\n",
    "            # Consume content and split lines\n",
    "            content = open(join_path(l_path, file_name), \"r\").read() \n",
    "\n",
    "            # Write content in output file\n",
    "            out_file.write(content)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load corpus in memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_corpus(data_dir, as_np_array=False):\n",
    "    corpus = {}\n",
    "    for file_name in listdir(data_dir):\n",
    "        path = join_path(data_dir, file_name)\n",
    "        if os.path.isfile(path):\n",
    "            file = open(path, \"r+\")\n",
    "            l_pair = file_name[:5]\n",
    "            if l_pair not in corpus:\n",
    "                corpus[l_pair] = {}\n",
    "            if as_np_array:\n",
    "                corpus[l_pair][file_name] = np.array(file.read().split('\\n'), dtype=object)\n",
    "            else:\n",
    "                corpus[l_pair][file_name[:3]] = file.read().split('\\n')\n",
    "            file.close()\n",
    "            remove(path)\n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = load_corpus(output_data_dir, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Delete short lines and lines with xml tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "2000491\n2000491\n1910858\n1910858\n1958593\n1958593\n"
    }
   ],
   "source": [
    "for pair, l_data in corpus.items():\n",
    "\n",
    "    # Find special lines\n",
    "    empty_lines = np.array([], dtype=int)\n",
    "    for _, value in l_data.items():\n",
    "        is_empty = np.vectorize(lambda x: len(x) <= 7 or x[0] == \"<\" or x[0] == \"(\")\n",
    "        indices = np.nonzero(is_empty(value))[0]\n",
    "        empty_lines = np.concatenate((empty_lines, indices))\n",
    "    empty_lines = np.unique(empty_lines)\n",
    "\n",
    "    # Remove lines with an xml tag or an empty char\n",
    "    for key, value in l_data.items():\n",
    "        new_corpus = np.delete(value, empty_lines)\n",
    "        corpus[pair][key] = new_corpus.tolist()\n",
    "        print(len(new_corpus))\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Write new file(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "for lang_pair, pair_dict in corpus.items():\n",
    "    for file_name, data in pair_dict.items():\n",
    "        file = open(join_path(output_data_dir, file_name), 'w')\n",
    "        file.write(\"\\n\".join(data))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The following steps must be run after executing the multialign script and the BPE tokenization script"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Join language files and add translation token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenized_dir = './data/tokenized/'\n",
    "output_dir = './data/'\n",
    "\n",
    "for sub_set in listdir(tokenized_dir):\n",
    "    sub_set_path = join_path(tokenized_dir, sub_set)\n",
    "\n",
    "    corpus = {}\n",
    "    src_data = []\n",
    "    tgt_data = []\n",
    "\n",
    "    for file_name in listdir(sub_set_path):\n",
    "        file_path = join_path(sub_set_path, file_name)\n",
    "        file = open(file_path, 'r')\n",
    "        corpus[file_name[:-4]] = file.read().split('\\n')\n",
    "\n",
    "    # An extra line will be created when loading the data to memory,\n",
    "    # that's why the -1 is there\n",
    "    for i in range(0, len(corpus['en']) - 1):\n",
    "        for src in corpus:\n",
    "            targets = filter(lambda x: x != src, corpus.keys())\n",
    "            for tgt in targets:\n",
    "                src_sentence = '_src_{}_tgt_{} '.format(src, tgt) + corpus[src][i]\n",
    "                src_data.append(src_sentence)\n",
    "                tgt_data.append(corpus[tgt][i])\n",
    "\n",
    "    out_src = open(join_path(output_dir, 'europarl_{}_src.txt'.format(sub_set)), 'w')\n",
    "    out_src.write(\"\\n\".join(src_data))\n",
    "    out_src = open(join_path(output_dir, 'europarl_{}_tgt.txt'.format(sub_set)), 'w')\n",
    "    out_src.write(\"\\n\".join(tgt_data))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing classifier data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "location = './data/translated/'\n",
    "out_location = './data/'\n",
    "lang_pool = ['en', 'es', 'fr', 'de']\n",
    "back_translation_num = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Back-Translations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Starting loops for: fr_es.txt\n",
      "Elapsed time to generate T0 for es to en: 207.33868741989136s\n",
      "Elapsed time to generate Back0 for en to es: 205.3305778503418s\n",
      "Elapsed time to generate T1 for es to en: 198.34174418449402s\n",
      "Elapsed time to generate Back1 for en to es: 196.4911506175995s\n",
      "Elapsed time to generate T2 for es to en: 189.56953811645508s\n",
      "Elapsed time to generate T0 for es to fr: 233.66205096244812s\n",
      "Elapsed time to generate Back0 for fr to es: 213.88674139976501s\n",
      "Elapsed time to generate T1 for es to fr: 216.71543908119202s\n",
      "Elapsed time to generate Back1 for fr to es: 203.10585403442383s\n",
      "Elapsed time to generate T2 for es to fr: 211.56110501289368s\n",
      "Elapsed time to generate T0 for es to de: 207.34401178359985s\n",
      "Elapsed time to generate Back0 for de to es: 203.3491015434265s\n",
      "Elapsed time to generate T1 for es to de: 191.28350114822388s\n",
      "Elapsed time to generate Back1 for de to es: 191.35160493850708s\n",
      "Elapsed time to generate T2 for es to de: 183.46519470214844s\n",
      "Starting loops for: fr_de.txt\n",
      "Elapsed time to generate T0 for de to en: 207.74458003044128s\n",
      "Elapsed time to generate Back0 for en to de: 201.59457302093506s\n",
      "Elapsed time to generate T1 for de to en: 192.8924219608307s\n",
      "Elapsed time to generate Back1 for en to de: 188.74455404281616s\n",
      "Elapsed time to generate T2 for de to en: 186.24185967445374s\n",
      "Elapsed time to generate T0 for de to es: 212.9380280971527s\n",
      "Elapsed time to generate Back0 for es to de: 197.15704894065857s\n",
      "Elapsed time to generate T1 for de to es: 197.6275064945221s\n",
      "Elapsed time to generate Back1 for es to de: 184.46945214271545s\n",
      "Elapsed time to generate T2 for de to es: 186.9193398952484s\n",
      "Elapsed time to generate T0 for de to fr: 229.1927421092987s\n",
      "Elapsed time to generate Back0 for fr to de: 204.63220977783203s\n",
      "Elapsed time to generate T1 for de to fr: 211.72729396820068s\n",
      "Elapsed time to generate Back1 for fr to de: 190.68727231025696s\n",
      "Elapsed time to generate T2 for de to fr: 203.47651195526123s\n",
      "Starting loops for: es_fr.txt\n",
      "Elapsed time to generate T0 for fr to en: 210.36058688163757s\n",
      "Elapsed time to generate Back0 for en to fr: 217.91536784172058s\n",
      "Elapsed time to generate T1 for fr to en: 196.24322152137756s\n",
      "Elapsed time to generate Back1 for en to fr: 206.4515562057495s\n",
      "Elapsed time to generate T2 for fr to en: 190.09330558776855s\n",
      "Elapsed time to generate T0 for fr to es: 213.2300820350647s\n",
      "Elapsed time to generate Back0 for es to fr: 216.35611009597778s\n",
      "Elapsed time to generate T1 for fr to es: 204.41878747940063s\n",
      "Elapsed time to generate Back1 for es to fr: 211.2978434562683s\n",
      "Elapsed time to generate T2 for fr to es: 197.29564452171326s\n",
      "Elapsed time to generate T0 for fr to de: 207.49643111228943s\n",
      "Elapsed time to generate Back0 for de to fr: 211.9428858757019s\n",
      "Elapsed time to generate T1 for fr to de: 191.94395422935486s\n",
      "Elapsed time to generate Back1 for de to fr: 199.52797651290894s\n",
      "Elapsed time to generate T2 for fr to de: 181.32560014724731s\n",
      "Starting loops for: de_en.txt\n",
      "Elapsed time to generate T0 for en to es: 207.9717676639557s\n",
      "Elapsed time to generate Back0 for es to en: 193.7153284549713s\n",
      "Elapsed time to generate T1 for en to es: 193.375066280365s\n",
      "Elapsed time to generate Back1 for es to en: 186.64630961418152s\n",
      "Elapsed time to generate T2 for en to es: 187.18723964691162s\n",
      "Elapsed time to generate T0 for en to fr: 224.45851516723633s\n",
      "Elapsed time to generate Back0 for fr to en: 199.66443252563477s\n",
      "Elapsed time to generate T1 for en to fr: 206.07215690612793s\n",
      "Elapsed time to generate Back1 for fr to en: 194.0488908290863s\n",
      "Elapsed time to generate T2 for en to fr: 201.66741943359375s\n",
      "Elapsed time to generate T0 for en to de: 200.64372301101685s\n",
      "Elapsed time to generate Back0 for de to en: 192.60541915893555s\n",
      "Elapsed time to generate T1 for en to de: 183.1745958328247s\n",
      "Elapsed time to generate Back1 for de to en: 185.5486981868744s\n",
      "Elapsed time to generate T2 for en to de: 177.95188188552856s\n",
      "Starting loops for: fr_en.txt\n",
      "Elapsed time to generate T0 for en to es: 213.84579706192017s\n",
      "Elapsed time to generate Back0 for es to en: 199.66039299964905s\n",
      "Elapsed time to generate T1 for en to es: 196.89717507362366s\n",
      "Elapsed time to generate Back1 for es to en: 193.04100441932678s\n",
      "Elapsed time to generate T2 for en to es: 195.74480175971985s\n",
      "Elapsed time to generate T0 for en to fr: 231.17321157455444s\n",
      "Elapsed time to generate Back0 for fr to en: 206.41378211975098s\n",
      "Elapsed time to generate T1 for en to fr: 215.1990807056427s\n",
      "Elapsed time to generate Back1 for fr to en: 197.68421387672424s\n",
      "Elapsed time to generate T2 for en to fr: 209.46006560325623s\n",
      "Elapsed time to generate T0 for en to de: 207.16357374191284s\n",
      "Elapsed time to generate Back0 for de to en: 201.08455967903137s\n",
      "Elapsed time to generate T1 for en to de: 193.06346464157104s\n",
      "Elapsed time to generate Back1 for de to en: 210.70783948898315s\n",
      "Elapsed time to generate T2 for en to de: 250.99977326393127s\n",
      "Starting loops for: de_es.txt\n",
      "Elapsed time to generate T0 for es to en: 273.39264273643494s\n",
      "Elapsed time to generate Back0 for en to es: 264.5250129699707s\n",
      "Elapsed time to generate T1 for es to en: 254.6147871017456s\n",
      "Elapsed time to generate Back1 for en to es: 250.91091084480286s\n",
      "Elapsed time to generate T2 for es to en: 243.0722017288208s\n",
      "Elapsed time to generate T0 for es to fr: 298.938672542572s\n",
      "Elapsed time to generate Back0 for fr to es: 271.5222907066345s\n",
      "Elapsed time to generate T1 for es to fr: 277.23047161102295s\n",
      "Elapsed time to generate Back1 for fr to es: 257.5011203289032s\n",
      "Elapsed time to generate T2 for es to fr: 246.97835230827332s\n",
      "Elapsed time to generate T0 for es to de: 200.95063829421997s\n",
      "Elapsed time to generate Back0 for de to es: 197.08390188217163s\n",
      "Elapsed time to generate T1 for es to de: 187.04641199111938s\n",
      "Elapsed time to generate Back1 for de to es: 187.48014879226685s\n",
      "Elapsed time to generate T2 for es to de: 180.9287941455841s\n",
      "Starting loops for: es_en.txt\n",
      "Elapsed time to generate T0 for en to es: 209.29935693740845s\n",
      "Elapsed time to generate Back0 for es to en: 201.31268620491028s\n",
      "Elapsed time to generate T1 for en to es: 199.25777387619019s\n",
      "Elapsed time to generate Back1 for es to en: 194.4949231147766s\n",
      "Elapsed time to generate T2 for en to es: 194.20461106300354s\n",
      "Elapsed time to generate T0 for en to fr: 230.6462173461914s\n",
      "Elapsed time to generate Back0 for fr to en: 204.54938912391663s\n",
      "Elapsed time to generate T1 for en to fr: 210.99067902565002s\n",
      "Elapsed time to generate Back1 for fr to en: 198.10011315345764s\n",
      "Elapsed time to generate T2 for en to fr: 206.21846342086792s\n",
      "Elapsed time to generate T0 for en to de: 204.24653935432434s\n",
      "Elapsed time to generate Back0 for de to en: 198.5111858844757s\n",
      "Elapsed time to generate T1 for en to de: 187.1501064300537s\n",
      "Elapsed time to generate Back1 for de to en: 184.96042490005493s\n",
      "Elapsed time to generate T2 for en to de: 180.15581560134888s\n",
      "Starting loops for: en_es.txt\n",
      "Elapsed time to generate T0 for es to en: 207.3231484889984s\n",
      "Elapsed time to generate Back0 for en to es: 200.2557966709137s\n",
      "Elapsed time to generate T1 for es to en: 197.36557173728943s\n",
      "Elapsed time to generate Back1 for en to es: 195.85658717155457s\n",
      "Elapsed time to generate T2 for es to en: 188.49116158485413s\n",
      "Elapsed time to generate T0 for es to fr: 230.0111756324768s\n",
      "Elapsed time to generate Back0 for fr to es: 207.86872053146362s\n",
      "Elapsed time to generate T1 for es to fr: 213.01162314414978s\n",
      "Elapsed time to generate Back1 for fr to es: 202.2572431564331s\n",
      "Elapsed time to generate T2 for es to fr: 207.00603437423706s\n",
      "Elapsed time to generate T0 for es to de: 205.2168254852295s\n",
      "Elapsed time to generate Back0 for de to es: 200.1473286151886s\n",
      "Elapsed time to generate T1 for es to de: 186.4070725440979s\n",
      "Elapsed time to generate Back1 for de to es: 189.55026960372925s\n",
      "Elapsed time to generate T2 for es to de: 179.22346234321594s\n",
      "Starting loops for: es_de.txt\n",
      "Elapsed time to generate T0 for de to en: 205.78579473495483s\n",
      "Elapsed time to generate Back0 for en to de: 192.1534924507141s\n",
      "Elapsed time to generate T1 for de to en: 188.7444610595703s\n",
      "Elapsed time to generate Back1 for en to de: 185.08099126815796s\n",
      "Elapsed time to generate T2 for de to en: 179.67459440231323s\n",
      "Elapsed time to generate T0 for de to es: 208.02965760231018s\n",
      "Elapsed time to generate Back0 for es to de: 191.37410426139832s\n",
      "Elapsed time to generate T1 for de to es: 191.96175169944763s\n",
      "Elapsed time to generate Back1 for es to de: 180.3058741092682s\n",
      "Elapsed time to generate T2 for de to es: 183.07608366012573s\n",
      "Elapsed time to generate T0 for de to fr: 224.6912841796875s\n",
      "Elapsed time to generate Back0 for fr to de: 197.54854679107666s\n",
      "Elapsed time to generate T1 for de to fr: 204.78016877174377s\n",
      "Elapsed time to generate Back1 for fr to de: 188.6601140499115s\n",
      "Elapsed time to generate T2 for de to fr: 194.0463879108429s\n",
      "Starting loops for: en_de.txt\n",
      "Elapsed time to generate T0 for de to en: 209.33749151229858s\n",
      "Elapsed time to generate Back0 for en to de: 196.44120502471924s\n",
      "Elapsed time to generate T1 for de to en: 190.78843450546265s\n",
      "Elapsed time to generate Back1 for en to de: 189.84886050224304s\n",
      "Elapsed time to generate T2 for de to en: 184.28184533119202s\n",
      "Elapsed time to generate T0 for de to es: 215.68278050422668s\n",
      "Elapsed time to generate Back0 for es to de: 194.93040680885315s\n",
      "Elapsed time to generate T1 for de to es: 191.05131149291992s\n",
      "Elapsed time to generate Back1 for es to de: 182.14468812942505s\n",
      "Elapsed time to generate T2 for de to es: 182.88596177101135s\n",
      "Elapsed time to generate T0 for de to fr: 230.87827277183533s\n",
      "Elapsed time to generate Back0 for fr to de: 197.10566353797913s\n",
      "Elapsed time to generate T1 for de to fr: 206.02131485939026s\n",
      "Elapsed time to generate Back1 for fr to de: 187.4513430595398s\n",
      "Elapsed time to generate T2 for de to fr: 199.63724899291992s\n",
      "Starting loops for: de_fr.txt\n",
      "Elapsed time to generate T0 for fr to en: 206.0875837802887s\n",
      "Elapsed time to generate Back0 for en to fr: 214.7891867160797s\n",
      "Elapsed time to generate T1 for fr to en: 194.68523979187012s\n",
      "Elapsed time to generate Back1 for en to fr: 205.43906044960022s\n",
      "Elapsed time to generate T2 for fr to en: 187.87144374847412s\n",
      "Elapsed time to generate T0 for fr to es: 213.96585988998413s\n",
      "Elapsed time to generate Back0 for es to fr: 216.92420625686646s\n",
      "Elapsed time to generate T1 for fr to es: 200.2914161682129s\n",
      "Elapsed time to generate Back1 for es to fr: 204.94666171073914s\n",
      "Elapsed time to generate T2 for fr to es: 192.0822036266327s\n",
      "Elapsed time to generate T0 for fr to de: 205.96928119659424s\n",
      "Elapsed time to generate Back0 for de to fr: 211.15920543670654s\n",
      "Elapsed time to generate T1 for fr to de: 193.73450326919556s\n",
      "Elapsed time to generate Back1 for de to fr: 200.94731044769287s\n",
      "Elapsed time to generate T2 for fr to de: 189.043954372406s\n",
      "Starting loops for: en_fr.txt\n",
      "Elapsed time to generate T0 for fr to en: 214.13701748847961s\n",
      "Elapsed time to generate Back0 for en to fr: 217.80672121047974s\n",
      "Elapsed time to generate T1 for fr to en: 198.56296825408936s\n",
      "Elapsed time to generate Back1 for en to fr: 207.3100244998932s\n",
      "Elapsed time to generate T2 for fr to en: 192.93356132507324s\n",
      "Elapsed time to generate T0 for fr to es: 216.01128602027893s\n",
      "Elapsed time to generate Back0 for es to fr: 219.32801723480225s\n",
      "Elapsed time to generate T1 for fr to es: 201.41867423057556s\n",
      "Elapsed time to generate Back1 for es to fr: 209.54786682128906s\n",
      "Elapsed time to generate T2 for fr to es: 193.91277837753296s\n",
      "Elapsed time to generate T0 for fr to de: 209.84370470046997s\n",
      "Elapsed time to generate Back0 for de to fr: 214.5276780128479s\n",
      "Elapsed time to generate T1 for fr to de: 191.5870292186737s\n",
      "Elapsed time to generate Back1 for de to fr: 197.8919780254364s\n",
      "Elapsed time to generate T2 for fr to de: 182.66338181495667s\n"
     ]
    }
   ],
   "source": [
    "ignore = []\n",
    "\n",
    "def translate_text(text, src, tgt, file_name, dir_path):\n",
    "    start = time.time()\n",
    "    response = requests.post('http://localhost:8080/translate', data = {'text': text, 'src': src, 'tgt': tgt})\n",
    "    end = time.time()\n",
    "    print('Elapsed time to generate {} for {} to {}: {}s'.format(file_name, src, tgt, end - start))\n",
    "    res_file = open(join_path(dir_path, '{}.txt'.format(file_name)), 'w')\n",
    "    res_file.write(response.text)\n",
    "    return response.text\n",
    "\n",
    "for file_name in listdir(location):\n",
    "    path = join_path(location, file_name)\n",
    "    dir_path = path[:-4]\n",
    "    if os.path.isfile(path) and file_name[:-4] not in ignore:\n",
    "        print(\"Starting loops for:\", file_name)\n",
    "        original_lang = file_name[:2]\n",
    "        translated_lang = file_name[-6:-4]\n",
    "\n",
    "        if not os.path.isdir(dir_path):\n",
    "            mkdir(dir_path)\n",
    "\n",
    "        content = open(path, 'r').read()\n",
    "        \n",
    "        for tgt in filter(lambda x: x != translated_lang, lang_pool):\n",
    "            tgt_dir = join_path(dir_path, tgt)\n",
    "            if not os.path.isdir(tgt_dir):\n",
    "                mkdir(tgt_dir)\n",
    "\n",
    "            last_back = content\n",
    "            for i in range(0, back_translation_num):\n",
    "                translation = translate_text(last_back, translated_lang, tgt, 'T{}'.format(i), tgt_dir)\n",
    "                last_back = translate_text(translation, tgt, translated_lang, 'Back{}'.format(i), tgt_dir)\n",
    "            translate_text(last_back, translated_lang, tgt, 'T{}'.format(back_translation_num), tgt_dir)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate BLEU-Score Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns = ['T{}-T{}_{}'.format(i, i+1, lang) for lang in lang_pool for i in range(\n",
    "    0, back_translation_num)] + ['src', 'origin', 'len']\n",
    "data = np.zeros((4*back_translation_num+3))\n",
    "tokenizer = Tokenizer('conservative')\n",
    "smoothing = SmoothingFunction()\n",
    "score_func = lambda ref, hyp: sentence_bleu([ref], hyp, smoothing_function=smoothing.method4, weights=(0.50, 0.35, 0.10, 0.05))\n",
    "#score_func = lambda ref, hyp: meteor_score([ref], hyp)\n",
    "\n",
    "for directory in [d for d in os.listdir(location) if os.path.isdir(join_path(location, d))]:\n",
    "    path = join_path(location, directory)\n",
    "\n",
    "    # Get the original file_size\n",
    "    src_file = open(join_path(location, \"{}.txt\".format(directory)), 'r')\n",
    "    src_sentences = src_file.read().split('\\n')\n",
    "    file_size = len(src_sentences)\n",
    "\n",
    "    bleu_data = np.zeros((file_size, 4*back_translation_num))\n",
    "\n",
    "    # Calculate BLEU Score for each sentence and its back-translations\n",
    "    for lang_idx, lang in enumerate(lang_pool):\n",
    "        lang_path = join_path(path, lang)\n",
    "        if not os.path.isdir(lang_path):\n",
    "            pass\n",
    "        else:\n",
    "            #sentences = [src_sentences]\n",
    "            sentences = []\n",
    "            for i in range(0, back_translation_num + 1):\n",
    "                file_name = 'T{}.txt'.format(i)\n",
    "                file_data = open(join_path(lang_path, file_name),\n",
    "                                 'r').read().split('\\n')\n",
    "                sentences.append(file_data)\n",
    "\n",
    "            for i in range(0, file_size):\n",
    "                for j in range(0, back_translation_num):\n",
    "                    ref_sentence, _ = tokenizer.tokenize(sentences[j][i])\n",
    "                    hypothesis, _ = tokenizer.tokenize(sentences[j+1][i])\n",
    "                    #ref_sentence = sentences[j][i]\n",
    "                    #hypothesis = sentences[j+1][i]\n",
    "                    bleu_data[i, lang_idx * back_translation_num +\n",
    "                        j] = score_func(ref_sentence, hypothesis) if len(hypothesis) > 3 else 0\n",
    "\n",
    "    # Append src and origin data\n",
    "    bleu_data = np.hstack(\n",
    "        (bleu_data, np.full((file_size, 1), lang_pool.index(directory[-2:]))))\n",
    "    bleu_data = np.hstack(\n",
    "        (bleu_data, np.full((file_size, 1), lang_pool.index(directory[:2]))))\n",
    "\n",
    "    # Append sentence length data fron the src file\n",
    "    src_sentences = open(join_path(location, '{}.txt'.format(directory)), 'r').read().split('\\n')\n",
    "    tok_sentences = [tokenizer.tokenize(s)[0] for s in src_sentences]\n",
    "    lengths = [len(ts) for ts in tok_sentences]\n",
    "    bleu_data = np.hstack((bleu_data, np.array([lengths]).T))\n",
    "    \n",
    "    data = np.vstack((data, bleu_data))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "(29071, 11)"
      ]
     },
     "metadata": {},
     "execution_count": 25
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data = np.delete(data, (0), axis=0)\n",
    "dataFrame = pd.DataFrame(final_data, index=range(0, final_data.shape[0]), columns=columns)\n",
    "dataFrame['src'] = dataFrame['src'].apply(lambda x: lang_pool[int(x)])\n",
    "dataFrame['origin'] = dataFrame['origin'].apply(lambda x: lang_pool[int(x)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "   T0-T1_en  T1-T2_en  T0-T1_es  T1-T2_es  T0-T1_fr  T1-T2_fr  T0-T1_de  \\\n",
       "0  0.862555  1.000000  0.874556  1.000000       0.0       0.0  0.811063   \n",
       "1  0.762768  0.986252  0.816364  0.927906       0.0       0.0  0.767965   \n",
       "2  0.828334  0.984621  0.873651  0.973038       0.0       0.0  0.780815   \n",
       "3  0.923626  0.976716  0.783705  0.894062       0.0       0.0  0.784267   \n",
       "4  0.865745  0.971507  0.813647  0.972308       0.0       0.0  0.821942   \n",
       "\n",
       "   T1-T2_de src origin    len  \n",
       "0  0.937282  fr     en  141.0  \n",
       "1  0.865177  fr     en  137.0  \n",
       "2  0.927165  fr     en  121.0  \n",
       "3  0.903629  fr     en  117.0  \n",
       "4  0.870699  fr     en  121.0  "
      ],
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>T0-T1_en</th>\n      <th>T1-T2_en</th>\n      <th>T0-T1_es</th>\n      <th>T1-T2_es</th>\n      <th>T0-T1_fr</th>\n      <th>T1-T2_fr</th>\n      <th>T0-T1_de</th>\n      <th>T1-T2_de</th>\n      <th>src</th>\n      <th>origin</th>\n      <th>len</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0.862555</td>\n      <td>1.000000</td>\n      <td>0.874556</td>\n      <td>1.000000</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.811063</td>\n      <td>0.937282</td>\n      <td>fr</td>\n      <td>en</td>\n      <td>141.0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0.762768</td>\n      <td>0.986252</td>\n      <td>0.816364</td>\n      <td>0.927906</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.767965</td>\n      <td>0.865177</td>\n      <td>fr</td>\n      <td>en</td>\n      <td>137.0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>0.828334</td>\n      <td>0.984621</td>\n      <td>0.873651</td>\n      <td>0.973038</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.780815</td>\n      <td>0.927165</td>\n      <td>fr</td>\n      <td>en</td>\n      <td>121.0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>0.923626</td>\n      <td>0.976716</td>\n      <td>0.783705</td>\n      <td>0.894062</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.784267</td>\n      <td>0.903629</td>\n      <td>fr</td>\n      <td>en</td>\n      <td>117.0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0.865745</td>\n      <td>0.971507</td>\n      <td>0.813647</td>\n      <td>0.972308</td>\n      <td>0.0</td>\n      <td>0.0</td>\n      <td>0.821942</td>\n      <td>0.870699</td>\n      <td>fr</td>\n      <td>en</td>\n      <td>121.0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "execution_count": 28
    }
   ],
   "source": [
    "dataFrame.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataFrame.to_csv(join_path(out_location, 'bleu_table_iter_weighted.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "Python 3.8.5 64-bit ('venv')",
   "display_name": "Python 3.8.5 64-bit ('venv')",
   "metadata": {
    "interpreter": {
     "hash": "72367b81df62cacbe340dc3857adc6dad15848bdc784de8048814aca17ca7315"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}