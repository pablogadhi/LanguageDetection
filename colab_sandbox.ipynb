{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.2-final"
    },
    "orig_nbformat": 2,
    "kernelspec": {
      "name": "python38264bitlanguagedetectionpipenv382f6b637c4e47f9bb779a88a8777172",
      "display_name": "Python 3.8.2 64-bit ('LanguageDetection': pipenv)"
    },
    "colab": {
      "name": "colab_sandbox.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pablogadhi/LanguageDetection/blob/master/colab_sandbox.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kn1jfXR_ugxS",
        "colab_type": "text"
      },
      "source": [
        "#Environment Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q0b-cXd1uSBm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 364
        },
        "outputId": "55d6cbd3-1958-4cc8-9317-a904e5c19183"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sun Sep 20 16:03:08 2020       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 450.66       Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   43C    P0    28W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                 ERR! |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vdAOlqqQuNcY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "outputId": "2c7a7ba9-da23-4c07-a710-a5fd209e39e6"
      },
      "source": [
        "!git clone https://github.com/pablogadhi/LanguageDetection.git"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'LanguageDetection'...\n",
            "remote: Enumerating objects: 183, done.\u001b[K\n",
            "remote: Counting objects: 100% (183/183), done.\u001b[K\n",
            "remote: Compressing objects: 100% (139/139), done.\u001b[K\n",
            "remote: Total 183 (delta 92), reused 110 (delta 39), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (183/183), 500.65 KiB | 727.00 KiB/s, done.\n",
            "Resolving deltas: 100% (92/92), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-x6l-fpMuNcj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "10d7b205-c1d9-4740-88dd-e7cec75e7993"
      },
      "source": [
        "!pip install OpenNMT-py"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting OpenNMT-py\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/9f/20/40f8b722aa0e35e259c144b6ec2d684f1aea7de869cf586c67cfd6fe1c55/OpenNMT_py-1.2.0-py3-none-any.whl (195kB)\n",
            "\u001b[K     |████████████████████████████████| 204kB 2.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from OpenNMT-py) (3.13)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from OpenNMT-py) (1.6.0+cu101)\n",
            "Requirement already satisfied: tensorboard>=1.14 in /usr/local/lib/python3.6/dist-packages (from OpenNMT-py) (2.3.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from OpenNMT-py) (1.15.0)\n",
            "Collecting torchtext==0.4.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/43/94/929d6bd236a4fb5c435982a7eb9730b78dcd8659acf328fd2ef9de85f483/torchtext-0.4.0-py3-none-any.whl (53kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 5.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from OpenNMT-py) (4.41.1)\n",
            "Collecting configargparse\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/bb/79/3045743bb26ca2e44a1d317c37395462bfed82dbbd38e69a3280b63696ce/ConfigArgParse-1.2.3.tar.gz (42kB)\n",
            "\u001b[K     |████████████████████████████████| 51kB 3.6MB/s \n",
            "\u001b[?25hCollecting pyonmttok==1.*; platform_system == \"Linux\"\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/19/f1/637ae094c6cb0095d5faec4cfd4473734ffe590ea61e1f3ab8d4569699fd/pyonmttok-1.19.0-cp36-cp36m-manylinux1_x86_64.whl (2.5MB)\n",
            "\u001b[K     |████████████████████████████████| 2.5MB 7.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: flask in /usr/local/lib/python3.6/dist-packages (from OpenNMT-py) (1.1.2)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from OpenNMT-py) (0.16.0)\n",
            "Collecting waitress\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/26/d1/5209fb8c764497a592363c47054436a515b47b8c3e4970ddd7184f088857/waitress-1.4.4-py2.py3-none-any.whl (58kB)\n",
            "\u001b[K     |████████████████████████████████| 61kB 3.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch>=1.6.0->OpenNMT-py) (1.18.5)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=1.14->OpenNMT-py) (1.17.2)\n",
            "Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=1.14->OpenNMT-py) (3.12.4)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=1.14->OpenNMT-py) (1.7.0)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=1.14->OpenNMT-py) (2.23.0)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=1.14->OpenNMT-py) (0.10.0)\n",
            "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=1.14->OpenNMT-py) (1.32.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=1.14->OpenNMT-py) (1.0.1)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=1.14->OpenNMT-py) (50.3.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=1.14->OpenNMT-py) (0.4.1)\n",
            "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tensorboard>=1.14->OpenNMT-py) (0.35.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=1.14->OpenNMT-py) (3.2.2)\n",
            "Requirement already satisfied: itsdangerous>=0.24 in /usr/local/lib/python3.6/dist-packages (from flask->OpenNMT-py) (1.1.0)\n",
            "Requirement already satisfied: Jinja2>=2.10.1 in /usr/local/lib/python3.6/dist-packages (from flask->OpenNMT-py) (2.11.2)\n",
            "Requirement already satisfied: click>=5.1 in /usr/local/lib/python3.6/dist-packages (from flask->OpenNMT-py) (7.1.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=1.14->OpenNMT-py) (4.6)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=1.14->OpenNMT-py) (0.2.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=1.14->OpenNMT-py) (4.1.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard>=1.14->OpenNMT-py) (2020.6.20)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard>=1.14->OpenNMT-py) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard>=1.14->OpenNMT-py) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard>=1.14->OpenNMT-py) (2.10)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=1.14->OpenNMT-py) (1.3.0)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard>=1.14->OpenNMT-py) (1.7.0)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from Jinja2>=2.10.1->flask->OpenNMT-py) (1.1.1)\n",
            "Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.6/dist-packages (from rsa<5,>=3.1.4; python_version >= \"3\"->google-auth<2,>=1.6.3->tensorboard>=1.14->OpenNMT-py) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=1.14->OpenNMT-py) (3.1.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard>=1.14->OpenNMT-py) (3.1.0)\n",
            "Building wheels for collected packages: configargparse\n",
            "  Building wheel for configargparse (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for configargparse: filename=ConfigArgParse-1.2.3-cp36-none-any.whl size=19329 sha256=b338a297a80e0688ea30098c11de297210f981ca898a45d06848c5765dc5e7f8\n",
            "  Stored in directory: /root/.cache/pip/wheels/bd/d6/53/034032da9498bda2385cd50a51a289e88090b5da2d592b1fdf\n",
            "Successfully built configargparse\n",
            "Installing collected packages: torchtext, configargparse, pyonmttok, waitress, OpenNMT-py\n",
            "  Found existing installation: torchtext 0.3.1\n",
            "    Uninstalling torchtext-0.3.1:\n",
            "      Successfully uninstalled torchtext-0.3.1\n",
            "Successfully installed OpenNMT-py-1.2.0 configargparse-1.2.3 pyonmttok-1.19.0 torchtext-0.4.0 waitress-1.4.4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_I-Sa3hWucdx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "36ca5c17-5d93-456a-9727-91694e3a2cf4"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "%cd drive/My\\ Drive/Tesis"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n",
            "/content/drive/My Drive/Tesis\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2tQbz5Yhu1SY",
        "colab_type": "text"
      },
      "source": [
        "#Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CkOtW8aIvGzv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "bcbe8355-acd4-424a-ce77-6ff263e810d1"
      },
      "source": [
        "!../../../LanguageDetection/onmt_train.sh data/multi-europarl models/multi-europarl 200000 200000 4096 1600 models/multi-europarl_step_28800.pt"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[2020-09-20 16:06:28,531 INFO] Loading checkpoint from models/multi-europarl_step_28800.pt\n",
            "[2020-09-20 16:06:46,029 INFO] Loading vocab from checkpoint at models/multi-europarl_step_28800.pt.\n",
            "[2020-09-20 16:06:46,583 INFO]  * src vocab size = 97861\n",
            "[2020-09-20 16:06:46,583 INFO]  * tgt vocab size = 97874\n",
            "[2020-09-20 16:06:46,583 INFO] Building model...\n",
            "[2020-09-20 16:06:59,006 INFO] NMTModel(\n",
            "  (encoder): TransformerEncoder(\n",
            "    (embeddings): Embeddings(\n",
            "      (make_embedding): Sequential(\n",
            "        (emb_luts): Elementwise(\n",
            "          (0): Embedding(97861, 512, padding_idx=1)\n",
            "        )\n",
            "        (pe): PositionalEncoding(\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (transformer): ModuleList(\n",
            "      (0): TransformerEncoderLayer(\n",
            "        (self_attn): MultiHeadedAttention(\n",
            "          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (softmax): Softmax(dim=-1)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (feed_forward): PositionwiseFeedForward(\n",
            "          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
            "          (dropout_1): Dropout(p=0.1, inplace=False)\n",
            "          (relu): ReLU()\n",
            "          (dropout_2): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "      (1): TransformerEncoderLayer(\n",
            "        (self_attn): MultiHeadedAttention(\n",
            "          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (softmax): Softmax(dim=-1)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (feed_forward): PositionwiseFeedForward(\n",
            "          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
            "          (dropout_1): Dropout(p=0.1, inplace=False)\n",
            "          (relu): ReLU()\n",
            "          (dropout_2): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "      (2): TransformerEncoderLayer(\n",
            "        (self_attn): MultiHeadedAttention(\n",
            "          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (softmax): Softmax(dim=-1)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (feed_forward): PositionwiseFeedForward(\n",
            "          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
            "          (dropout_1): Dropout(p=0.1, inplace=False)\n",
            "          (relu): ReLU()\n",
            "          (dropout_2): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "      (3): TransformerEncoderLayer(\n",
            "        (self_attn): MultiHeadedAttention(\n",
            "          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (softmax): Softmax(dim=-1)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (feed_forward): PositionwiseFeedForward(\n",
            "          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
            "          (dropout_1): Dropout(p=0.1, inplace=False)\n",
            "          (relu): ReLU()\n",
            "          (dropout_2): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "      (4): TransformerEncoderLayer(\n",
            "        (self_attn): MultiHeadedAttention(\n",
            "          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (softmax): Softmax(dim=-1)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (feed_forward): PositionwiseFeedForward(\n",
            "          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
            "          (dropout_1): Dropout(p=0.1, inplace=False)\n",
            "          (relu): ReLU()\n",
            "          (dropout_2): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "      (5): TransformerEncoderLayer(\n",
            "        (self_attn): MultiHeadedAttention(\n",
            "          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (softmax): Softmax(dim=-1)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (feed_forward): PositionwiseFeedForward(\n",
            "          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
            "          (dropout_1): Dropout(p=0.1, inplace=False)\n",
            "          (relu): ReLU()\n",
            "          (dropout_2): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "    )\n",
            "    (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
            "  )\n",
            "  (decoder): TransformerDecoder(\n",
            "    (embeddings): Embeddings(\n",
            "      (make_embedding): Sequential(\n",
            "        (emb_luts): Elementwise(\n",
            "          (0): Embedding(97874, 512, padding_idx=1)\n",
            "        )\n",
            "        (pe): PositionalEncoding(\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (transformer_layers): ModuleList(\n",
            "      (0): TransformerDecoderLayer(\n",
            "        (self_attn): MultiHeadedAttention(\n",
            "          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (softmax): Softmax(dim=-1)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (context_attn): MultiHeadedAttention(\n",
            "          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (softmax): Softmax(dim=-1)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (feed_forward): PositionwiseFeedForward(\n",
            "          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
            "          (dropout_1): Dropout(p=0.1, inplace=False)\n",
            "          (relu): ReLU()\n",
            "          (dropout_2): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (layer_norm_1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
            "        (layer_norm_2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
            "        (drop): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "      (1): TransformerDecoderLayer(\n",
            "        (self_attn): MultiHeadedAttention(\n",
            "          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (softmax): Softmax(dim=-1)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (context_attn): MultiHeadedAttention(\n",
            "          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (softmax): Softmax(dim=-1)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (feed_forward): PositionwiseFeedForward(\n",
            "          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
            "          (dropout_1): Dropout(p=0.1, inplace=False)\n",
            "          (relu): ReLU()\n",
            "          (dropout_2): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (layer_norm_1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
            "        (layer_norm_2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
            "        (drop): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "      (2): TransformerDecoderLayer(\n",
            "        (self_attn): MultiHeadedAttention(\n",
            "          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (softmax): Softmax(dim=-1)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (context_attn): MultiHeadedAttention(\n",
            "          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (softmax): Softmax(dim=-1)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (feed_forward): PositionwiseFeedForward(\n",
            "          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
            "          (dropout_1): Dropout(p=0.1, inplace=False)\n",
            "          (relu): ReLU()\n",
            "          (dropout_2): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (layer_norm_1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
            "        (layer_norm_2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
            "        (drop): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "      (3): TransformerDecoderLayer(\n",
            "        (self_attn): MultiHeadedAttention(\n",
            "          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (softmax): Softmax(dim=-1)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (context_attn): MultiHeadedAttention(\n",
            "          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (softmax): Softmax(dim=-1)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (feed_forward): PositionwiseFeedForward(\n",
            "          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
            "          (dropout_1): Dropout(p=0.1, inplace=False)\n",
            "          (relu): ReLU()\n",
            "          (dropout_2): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (layer_norm_1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
            "        (layer_norm_2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
            "        (drop): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "      (4): TransformerDecoderLayer(\n",
            "        (self_attn): MultiHeadedAttention(\n",
            "          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (softmax): Softmax(dim=-1)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (context_attn): MultiHeadedAttention(\n",
            "          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (softmax): Softmax(dim=-1)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (feed_forward): PositionwiseFeedForward(\n",
            "          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
            "          (dropout_1): Dropout(p=0.1, inplace=False)\n",
            "          (relu): ReLU()\n",
            "          (dropout_2): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (layer_norm_1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
            "        (layer_norm_2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
            "        (drop): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "      (5): TransformerDecoderLayer(\n",
            "        (self_attn): MultiHeadedAttention(\n",
            "          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (softmax): Softmax(dim=-1)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (context_attn): MultiHeadedAttention(\n",
            "          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (softmax): Softmax(dim=-1)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (feed_forward): PositionwiseFeedForward(\n",
            "          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
            "          (dropout_1): Dropout(p=0.1, inplace=False)\n",
            "          (relu): ReLU()\n",
            "          (dropout_2): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (layer_norm_1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
            "        (layer_norm_2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
            "        (drop): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "    )\n",
            "    (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
            "  )\n",
            "  (generator): Sequential(\n",
            "    (0): Linear(in_features=512, out_features=97874, bias=True)\n",
            "    (1): Cast()\n",
            "    (2): LogSoftmax(dim=-1)\n",
            "  )\n",
            ")\n",
            "[2020-09-20 16:06:59,011 INFO] encoder: 69020160\n",
            "[2020-09-20 16:06:59,011 INFO] decoder: 125546066\n",
            "[2020-09-20 16:06:59,011 INFO] * number of parameters: 194566226\n",
            "[2020-09-20 16:07:00,412 INFO] Starting training on GPU: [0]\n",
            "[2020-09-20 16:07:00,412 INFO] Start training loop and validate every 200000 steps...\n",
            "[2020-09-20 16:07:00,412 INFO] Loading dataset from data/multi-europarl.train.0.pt\n",
            "[2020-09-20 16:07:25,398 INFO] number of examples: 427974\n",
            "[2020-09-20 16:08:24,785 INFO] Step 28850/200000; acc:  65.26; ppl:  4.54; xent: 1.51; lr: 0.00052; 4229/4237 tok/s;     84 sec\n",
            "[2020-09-20 16:09:20,247 INFO] Step 28900/200000; acc:  65.17; ppl:  4.50; xent: 1.51; lr: 0.00052; 6579/6553 tok/s;    140 sec\n",
            "[2020-09-20 16:10:15,065 INFO] Step 28950/200000; acc:  65.39; ppl:  4.45; xent: 1.49; lr: 0.00052; 6680/6497 tok/s;    195 sec\n",
            "[2020-09-20 16:11:10,511 INFO] Step 29000/200000; acc:  65.61; ppl:  4.41; xent: 1.48; lr: 0.00052; 6548/6569 tok/s;    250 sec\n",
            "[2020-09-20 16:12:05,414 INFO] Step 29050/200000; acc:  66.34; ppl:  4.21; xent: 1.44; lr: 0.00052; 6544/6668 tok/s;    305 sec\n",
            "[2020-09-20 16:13:00,574 INFO] Step 29100/200000; acc:  66.44; ppl:  4.23; xent: 1.44; lr: 0.00052; 6594/6496 tok/s;    360 sec\n",
            "[2020-09-20 16:13:55,965 INFO] Step 29150/200000; acc:  65.99; ppl:  4.27; xent: 1.45; lr: 0.00052; 6406/6480 tok/s;    416 sec\n",
            "[2020-09-20 16:14:50,993 INFO] Step 29200/200000; acc:  66.56; ppl:  4.12; xent: 1.42; lr: 0.00052; 6546/6575 tok/s;    471 sec\n",
            "[2020-09-20 16:15:46,346 INFO] Step 29250/200000; acc:  66.72; ppl:  4.11; xent: 1.41; lr: 0.00052; 6581/6582 tok/s;    526 sec\n",
            "[2020-09-20 16:16:41,272 INFO] Step 29300/200000; acc:  66.60; ppl:  4.11; xent: 1.41; lr: 0.00052; 6412/6491 tok/s;    581 sec\n",
            "[2020-09-20 16:17:35,691 INFO] Step 29350/200000; acc:  67.62; ppl:  3.91; xent: 1.36; lr: 0.00052; 6722/6563 tok/s;    635 sec\n",
            "[2020-09-20 16:18:31,918 INFO] Step 29400/200000; acc:  67.54; ppl:  3.92; xent: 1.37; lr: 0.00052; 6352/6649 tok/s;    692 sec\n",
            "[2020-09-20 16:19:27,118 INFO] Step 29450/200000; acc:  67.04; ppl:  4.00; xent: 1.39; lr: 0.00052; 6634/6602 tok/s;    747 sec\n",
            "[2020-09-20 16:20:21,413 INFO] Step 29500/200000; acc:  67.47; ppl:  3.91; xent: 1.36; lr: 0.00051; 6696/6522 tok/s;    801 sec\n",
            "[2020-09-20 16:21:17,264 INFO] Step 29550/200000; acc:  67.83; ppl:  3.85; xent: 1.35; lr: 0.00051; 6470/6630 tok/s;    857 sec\n",
            "[2020-09-20 16:22:12,850 INFO] Step 29600/200000; acc:  67.52; ppl:  3.90; xent: 1.36; lr: 0.00051; 6603/6550 tok/s;    912 sec\n",
            "[2020-09-20 16:23:07,867 INFO] Step 29650/200000; acc:  67.82; ppl:  3.83; xent: 1.34; lr: 0.00051; 6664/6565 tok/s;    967 sec\n",
            "[2020-09-20 16:24:02,498 INFO] Step 29700/200000; acc:  67.94; ppl:  3.79; xent: 1.33; lr: 0.00051; 6554/6551 tok/s;   1022 sec\n",
            "[2020-09-20 16:24:57,845 INFO] Step 29750/200000; acc:  67.88; ppl:  3.79; xent: 1.33; lr: 0.00051; 6632/6592 tok/s;   1077 sec\n",
            "[2020-09-20 16:25:51,894 INFO] Step 29800/200000; acc:  67.97; ppl:  3.76; xent: 1.32; lr: 0.00051; 6656/6564 tok/s;   1131 sec\n",
            "[2020-09-20 16:26:46,928 INFO] Step 29850/200000; acc:  68.59; ppl:  3.67; xent: 1.30; lr: 0.00051; 6345/6499 tok/s;   1187 sec\n",
            "[2020-09-20 16:27:42,419 INFO] Step 29900/200000; acc:  68.21; ppl:  3.71; xent: 1.31; lr: 0.00051; 6482/6650 tok/s;   1242 sec\n",
            "[2020-09-20 16:28:37,669 INFO] Step 29950/200000; acc:  68.35; ppl:  3.69; xent: 1.31; lr: 0.00051; 6432/6445 tok/s;   1297 sec\n",
            "[2020-09-20 16:29:33,139 INFO] Step 30000/200000; acc:  68.52; ppl:  3.64; xent: 1.29; lr: 0.00051; 6527/6620 tok/s;   1353 sec\n",
            "[2020-09-20 16:30:27,644 INFO] Step 30050/200000; acc:  68.37; ppl:  3.66; xent: 1.30; lr: 0.00051; 6571/6586 tok/s;   1407 sec\n",
            "[2020-09-20 16:31:23,096 INFO] Step 30100/200000; acc:  68.87; ppl:  3.59; xent: 1.28; lr: 0.00051; 6551/6561 tok/s;   1463 sec\n",
            "[2020-09-20 16:32:17,476 INFO] Step 30150/200000; acc:  68.74; ppl:  3.60; xent: 1.28; lr: 0.00051; 6631/6555 tok/s;   1517 sec\n",
            "[2020-09-20 16:33:12,776 INFO] Step 30200/200000; acc:  69.00; ppl:  3.55; xent: 1.27; lr: 0.00051; 6629/6646 tok/s;   1572 sec\n",
            "[2020-09-20 16:34:07,087 INFO] Step 30250/200000; acc:  69.02; ppl:  3.54; xent: 1.26; lr: 0.00051; 6598/6540 tok/s;   1627 sec\n",
            "[2020-09-20 16:35:02,152 INFO] Step 30300/200000; acc:  69.53; ppl:  3.47; xent: 1.24; lr: 0.00051; 6609/6591 tok/s;   1682 sec\n",
            "[2020-09-20 16:35:13,367 INFO] Loading dataset from data/multi-europarl.train.1.pt\n",
            "[2020-09-20 16:35:38,526 INFO] number of examples: 426663\n",
            "[2020-09-20 16:36:27,341 INFO] Step 30350/200000; acc:  66.13; ppl:  4.26; xent: 1.45; lr: 0.00051; 4191/4277 tok/s;   1767 sec\n",
            "[2020-09-20 16:37:23,014 INFO] Step 30400/200000; acc:  65.73; ppl:  4.39; xent: 1.48; lr: 0.00051; 6600/6601 tok/s;   1823 sec\n",
            "[2020-09-20 16:37:23,771 INFO] Saving checkpoint models/multi-europarl_step_30400.pt\n",
            "[2020-09-20 16:39:52,464 INFO] Step 30450/200000; acc:  65.89; ppl:  4.33; xent: 1.46; lr: 0.00051; 2432/2435 tok/s;   1972 sec\n",
            "[2020-09-20 16:40:47,997 INFO] Step 30500/200000; acc:  66.36; ppl:  4.23; xent: 1.44; lr: 0.00051; 6445/6552 tok/s;   2028 sec\n",
            "[2020-09-20 16:41:43,180 INFO] Step 30550/200000; acc:  66.25; ppl:  4.26; xent: 1.45; lr: 0.00051; 6520/6502 tok/s;   2083 sec\n",
            "[2020-09-20 16:42:37,576 INFO] Step 30600/200000; acc:  66.34; ppl:  4.22; xent: 1.44; lr: 0.00051; 6758/6590 tok/s;   2137 sec\n",
            "[2020-09-20 16:43:32,208 INFO] Step 30650/200000; acc:  66.73; ppl:  4.11; xent: 1.41; lr: 0.00050; 6543/6610 tok/s;   2192 sec\n",
            "[2020-09-20 16:44:27,414 INFO] Step 30700/200000; acc:  66.96; ppl:  4.06; xent: 1.40; lr: 0.00050; 6406/6496 tok/s;   2247 sec\n",
            "[2020-09-20 16:45:22,193 INFO] Step 30750/200000; acc:  66.84; ppl:  4.09; xent: 1.41; lr: 0.00050; 6754/6536 tok/s;   2302 sec\n",
            "[2020-09-20 16:46:17,364 INFO] Step 30800/200000; acc:  67.31; ppl:  3.97; xent: 1.38; lr: 0.00050; 6643/6655 tok/s;   2357 sec\n",
            "[2020-09-20 16:47:12,013 INFO] Step 30850/200000; acc:  67.19; ppl:  3.98; xent: 1.38; lr: 0.00050; 6553/6636 tok/s;   2412 sec\n",
            "[2020-09-20 16:48:07,316 INFO] Step 30900/200000; acc:  67.56; ppl:  3.88; xent: 1.36; lr: 0.00050; 6610/6646 tok/s;   2467 sec\n",
            "[2020-09-20 16:49:01,682 INFO] Step 30950/200000; acc:  67.56; ppl:  3.93; xent: 1.37; lr: 0.00050; 6526/6507 tok/s;   2521 sec\n",
            "[2020-09-20 16:49:57,089 INFO] Step 31000/200000; acc:  67.21; ppl:  3.96; xent: 1.38; lr: 0.00050; 6381/6477 tok/s;   2577 sec\n",
            "[2020-09-20 16:50:51,396 INFO] Step 31050/200000; acc:  67.62; ppl:  3.86; xent: 1.35; lr: 0.00050; 6694/6579 tok/s;   2631 sec\n",
            "[2020-09-20 16:51:46,324 INFO] Step 31100/200000; acc:  67.34; ppl:  3.91; xent: 1.36; lr: 0.00050; 6556/6544 tok/s;   2686 sec\n",
            "[2020-09-20 16:52:41,541 INFO] Step 31150/200000; acc:  68.20; ppl:  3.76; xent: 1.32; lr: 0.00050; 6654/6601 tok/s;   2741 sec\n",
            "[2020-09-20 16:53:36,733 INFO] Step 31200/200000; acc:  67.84; ppl:  3.81; xent: 1.34; lr: 0.00050; 6511/6569 tok/s;   2796 sec\n",
            "[2020-09-20 16:54:32,243 INFO] Step 31250/200000; acc:  68.27; ppl:  3.72; xent: 1.31; lr: 0.00050; 6525/6562 tok/s;   2852 sec\n",
            "[2020-09-20 16:55:26,274 INFO] Step 31300/200000; acc:  68.55; ppl:  3.67; xent: 1.30; lr: 0.00050; 6697/6545 tok/s;   2906 sec\n",
            "[2020-09-20 16:56:21,439 INFO] Step 31350/200000; acc:  68.53; ppl:  3.67; xent: 1.30; lr: 0.00050; 6478/6611 tok/s;   2961 sec\n",
            "[2020-09-20 16:57:16,819 INFO] Step 31400/200000; acc:  68.66; ppl:  3.64; xent: 1.29; lr: 0.00050; 6279/6590 tok/s;   3016 sec\n",
            "[2020-09-20 16:58:12,270 INFO] Step 31450/200000; acc:  68.60; ppl:  3.66; xent: 1.30; lr: 0.00050; 6529/6478 tok/s;   3072 sec\n",
            "[2020-09-20 16:59:06,959 INFO] Step 31500/200000; acc:  69.17; ppl:  3.54; xent: 1.27; lr: 0.00050; 6623/6627 tok/s;   3127 sec\n",
            "[2020-09-20 17:00:01,943 INFO] Step 31550/200000; acc:  68.41; ppl:  3.66; xent: 1.30; lr: 0.00050; 6648/6470 tok/s;   3182 sec\n",
            "[2020-09-20 17:00:57,641 INFO] Step 31600/200000; acc:  68.60; ppl:  3.62; xent: 1.29; lr: 0.00050; 6563/6593 tok/s;   3237 sec\n",
            "[2020-09-20 17:01:52,776 INFO] Step 31650/200000; acc:  69.02; ppl:  3.54; xent: 1.26; lr: 0.00050; 6525/6677 tok/s;   3292 sec\n",
            "[2020-09-20 17:02:48,250 INFO] Step 31700/200000; acc:  69.12; ppl:  3.53; xent: 1.26; lr: 0.00050; 6570/6613 tok/s;   3348 sec\n",
            "[2020-09-20 17:03:44,010 INFO] Step 31750/200000; acc:  69.36; ppl:  3.48; xent: 1.25; lr: 0.00050; 6522/6556 tok/s;   3404 sec\n",
            "[2020-09-20 17:04:39,301 INFO] Step 31800/200000; acc:  69.21; ppl:  3.49; xent: 1.25; lr: 0.00050; 6483/6568 tok/s;   3459 sec\n",
            "[2020-09-20 17:05:01,966 INFO] Loading dataset from data/multi-europarl.train.2.pt\n",
            "[2020-09-20 17:05:27,430 INFO] number of examples: 426350\n",
            "[2020-09-20 17:06:04,935 INFO] Step 31850/200000; acc:  67.10; ppl:  4.00; xent: 1.39; lr: 0.00050; 4271/4218 tok/s;   3545 sec\n",
            "[2020-09-20 17:07:00,389 INFO] Step 31900/200000; acc:  65.86; ppl:  4.33; xent: 1.47; lr: 0.00049; 6486/6667 tok/s;   3600 sec\n",
            "[2020-09-20 17:07:55,895 INFO] Step 31950/200000; acc:  66.06; ppl:  4.30; xent: 1.46; lr: 0.00049; 6607/6624 tok/s;   3655 sec\n",
            "[2020-09-20 17:08:50,706 INFO] Step 32000/200000; acc:  65.89; ppl:  4.31; xent: 1.46; lr: 0.00049; 6470/6415 tok/s;   3710 sec\n",
            "[2020-09-20 17:08:51,397 INFO] Saving checkpoint models/multi-europarl_step_32000.pt\n",
            "[2020-09-20 17:11:20,732 INFO] Step 32050/200000; acc:  66.41; ppl:  4.19; xent: 1.43; lr: 0.00049; 2443/2418 tok/s;   3860 sec\n",
            "[2020-09-20 17:12:16,533 INFO] Step 32100/200000; acc:  66.09; ppl:  4.24; xent: 1.44; lr: 0.00049; 6423/6545 tok/s;   3916 sec\n",
            "[2020-09-20 17:13:11,147 INFO] Step 32150/200000; acc:  66.98; ppl:  4.05; xent: 1.40; lr: 0.00049; 6695/6519 tok/s;   3971 sec\n",
            "[2020-09-20 17:14:05,672 INFO] Step 32200/200000; acc:  67.05; ppl:  4.03; xent: 1.39; lr: 0.00049; 6732/6574 tok/s;   4025 sec\n",
            "[2020-09-20 17:15:00,940 INFO] Step 32250/200000; acc:  66.74; ppl:  4.07; xent: 1.40; lr: 0.00049; 6402/6561 tok/s;   4081 sec\n",
            "[2020-09-20 17:15:56,234 INFO] Step 32300/200000; acc:  67.83; ppl:  3.87; xent: 1.35; lr: 0.00049; 6621/6618 tok/s;   4136 sec\n",
            "[2020-09-20 17:16:51,316 INFO] Step 32350/200000; acc:  67.49; ppl:  3.89; xent: 1.36; lr: 0.00049; 6457/6658 tok/s;   4191 sec\n",
            "[2020-09-20 17:17:46,580 INFO] Step 32400/200000; acc:  67.45; ppl:  3.91; xent: 1.36; lr: 0.00049; 6307/6537 tok/s;   4246 sec\n",
            "[2020-09-20 17:18:42,215 INFO] Step 32450/200000; acc:  67.90; ppl:  3.84; xent: 1.34; lr: 0.00049; 6304/6476 tok/s;   4302 sec\n",
            "[2020-09-20 17:19:37,490 INFO] Step 32500/200000; acc:  67.65; ppl:  3.87; xent: 1.35; lr: 0.00049; 6536/6516 tok/s;   4357 sec\n",
            "[2020-09-20 17:20:32,947 INFO] Step 32550/200000; acc:  68.26; ppl:  3.73; xent: 1.32; lr: 0.00049; 6481/6667 tok/s;   4413 sec\n",
            "[2020-09-20 17:21:27,852 INFO] Step 32600/200000; acc:  68.24; ppl:  3.74; xent: 1.32; lr: 0.00049; 6720/6554 tok/s;   4467 sec\n",
            "[2020-09-20 17:22:22,564 INFO] Step 32650/200000; acc:  67.62; ppl:  3.85; xent: 1.35; lr: 0.00049; 6540/6457 tok/s;   4522 sec\n",
            "[2020-09-20 17:23:17,547 INFO] Step 32700/200000; acc:  68.06; ppl:  3.77; xent: 1.33; lr: 0.00049; 6594/6471 tok/s;   4577 sec\n",
            "[2020-09-20 17:24:12,519 INFO] Step 32750/200000; acc:  68.14; ppl:  3.74; xent: 1.32; lr: 0.00049; 6561/6549 tok/s;   4632 sec\n",
            "[2020-09-20 17:25:07,998 INFO] Step 32800/200000; acc:  68.53; ppl:  3.67; xent: 1.30; lr: 0.00049; 6546/6525 tok/s;   4688 sec\n",
            "[2020-09-20 17:26:04,099 INFO] Step 32850/200000; acc:  68.63; ppl:  3.65; xent: 1.29; lr: 0.00049; 6539/6569 tok/s;   4744 sec\n",
            "[2020-09-20 17:26:59,615 INFO] Step 32900/200000; acc:  68.72; ppl:  3.64; xent: 1.29; lr: 0.00049; 6473/6594 tok/s;   4799 sec\n",
            "[2020-09-20 17:27:53,545 INFO] Step 32950/200000; acc:  68.47; ppl:  3.67; xent: 1.30; lr: 0.00049; 6754/6534 tok/s;   4853 sec\n",
            "[2020-09-20 17:28:48,918 INFO] Step 33000/200000; acc:  68.55; ppl:  3.64; xent: 1.29; lr: 0.00049; 6427/6478 tok/s;   4909 sec\n",
            "[2020-09-20 17:29:44,327 INFO] Step 33050/200000; acc:  69.00; ppl:  3.56; xent: 1.27; lr: 0.00049; 6573/6541 tok/s;   4964 sec\n",
            "[2020-09-20 17:30:39,042 INFO] Step 33100/200000; acc:  69.19; ppl:  3.51; xent: 1.26; lr: 0.00049; 6696/6553 tok/s;   5019 sec\n",
            "[2020-09-20 17:31:33,317 INFO] Step 33150/200000; acc:  69.58; ppl:  3.45; xent: 1.24; lr: 0.00049; 6724/6629 tok/s;   5073 sec\n",
            "[2020-09-20 17:32:28,929 INFO] Step 33200/200000; acc:  69.44; ppl:  3.47; xent: 1.24; lr: 0.00049; 6616/6622 tok/s;   5129 sec\n",
            "[2020-09-20 17:33:24,471 INFO] Step 33250/200000; acc:  69.47; ppl:  3.46; xent: 1.24; lr: 0.00048; 6468/6539 tok/s;   5184 sec\n",
            "[2020-09-20 17:34:18,768 INFO] Step 33300/200000; acc:  69.35; ppl:  3.49; xent: 1.25; lr: 0.00048; 6529/6439 tok/s;   5238 sec\n",
            "[2020-09-20 17:34:52,583 INFO] Loading dataset from data/multi-europarl.train.3.pt\n",
            "[2020-09-20 17:35:19,947 INFO] number of examples: 426711\n",
            "[2020-09-20 17:35:47,353 INFO] Step 33350/200000; acc:  68.16; ppl:  3.76; xent: 1.32; lr: 0.00048; 4054/4130 tok/s;   5327 sec\n",
            "[2020-09-20 17:36:43,043 INFO] Step 33400/200000; acc:  65.99; ppl:  4.30; xent: 1.46; lr: 0.00048; 6531/6623 tok/s;   5383 sec\n",
            "[2020-09-20 17:37:37,549 INFO] Step 33450/200000; acc:  65.87; ppl:  4.32; xent: 1.46; lr: 0.00048; 6564/6518 tok/s;   5437 sec\n",
            "[2020-09-20 17:38:33,278 INFO] Step 33500/200000; acc:  66.79; ppl:  4.09; xent: 1.41; lr: 0.00048; 6406/6572 tok/s;   5493 sec\n",
            "[2020-09-20 17:39:27,747 INFO] Step 33550/200000; acc:  66.84; ppl:  4.08; xent: 1.41; lr: 0.00048; 6750/6544 tok/s;   5547 sec\n",
            "[2020-09-20 17:40:22,813 INFO] Step 33600/200000; acc:  66.87; ppl:  4.08; xent: 1.41; lr: 0.00048; 6580/6626 tok/s;   5602 sec\n",
            "[2020-09-20 17:40:23,537 INFO] Saving checkpoint models/multi-europarl_step_33600.pt\n",
            "[2020-09-20 17:41:52,450 INFO] Step 33650/200000; acc:  67.14; ppl:  4.02; xent: 1.39; lr: 0.00048; 3901/4055 tok/s;   5692 sec\n",
            "[2020-09-20 17:42:48,533 INFO] Step 33700/200000; acc:  67.35; ppl:  3.96; xent: 1.38; lr: 0.00048; 6451/6467 tok/s;   5748 sec\n",
            "[2020-09-20 17:43:43,241 INFO] Step 33750/200000; acc:  67.52; ppl:  3.93; xent: 1.37; lr: 0.00048; 6675/6484 tok/s;   5803 sec\n",
            "[2020-09-20 17:44:38,488 INFO] Step 33800/200000; acc:  67.88; ppl:  3.85; xent: 1.35; lr: 0.00048; 6516/6570 tok/s;   5858 sec\n",
            "[2020-09-20 17:45:33,872 INFO] Step 33850/200000; acc:  67.97; ppl:  3.81; xent: 1.34; lr: 0.00048; 6533/6619 tok/s;   5913 sec\n",
            "[2020-09-20 17:46:29,115 INFO] Step 33900/200000; acc:  67.84; ppl:  3.82; xent: 1.34; lr: 0.00048; 6616/6635 tok/s;   5969 sec\n",
            "[2020-09-20 17:47:24,051 INFO] Step 33950/200000; acc:  68.13; ppl:  3.79; xent: 1.33; lr: 0.00048; 6566/6482 tok/s;   6024 sec\n",
            "[2020-09-20 17:48:18,472 INFO] Step 34000/200000; acc:  67.19; ppl:  3.94; xent: 1.37; lr: 0.00048; 6586/6566 tok/s;   6078 sec\n",
            "[2020-09-20 17:49:13,649 INFO] Step 34050/200000; acc:  68.38; ppl:  3.71; xent: 1.31; lr: 0.00048; 6636/6578 tok/s;   6133 sec\n",
            "[2020-09-20 17:50:09,309 INFO] Step 34100/200000; acc:  67.80; ppl:  3.80; xent: 1.34; lr: 0.00048; 6504/6546 tok/s;   6189 sec\n",
            "[2020-09-20 17:51:04,798 INFO] Step 34150/200000; acc:  68.48; ppl:  3.71; xent: 1.31; lr: 0.00048; 6531/6541 tok/s;   6244 sec\n",
            "[2020-09-20 17:51:59,713 INFO] Step 34200/200000; acc:  68.73; ppl:  3.63; xent: 1.29; lr: 0.00048; 6577/6557 tok/s;   6299 sec\n",
            "[2020-09-20 17:52:55,306 INFO] Step 34250/200000; acc:  68.36; ppl:  3.68; xent: 1.30; lr: 0.00048; 6571/6503 tok/s;   6355 sec\n",
            "[2020-09-20 17:53:51,045 INFO] Step 34300/200000; acc:  68.73; ppl:  3.61; xent: 1.28; lr: 0.00048; 6386/6556 tok/s;   6411 sec\n",
            "[2020-09-20 17:54:44,905 INFO] Step 34350/200000; acc:  69.05; ppl:  3.55; xent: 1.27; lr: 0.00048; 6655/6527 tok/s;   6464 sec\n",
            "[2020-09-20 17:55:40,628 INFO] Step 34400/200000; acc:  69.30; ppl:  3.51; xent: 1.25; lr: 0.00048; 6429/6636 tok/s;   6520 sec\n",
            "[2020-09-20 17:56:36,727 INFO] Step 34450/200000; acc:  69.27; ppl:  3.52; xent: 1.26; lr: 0.00048; 6224/6563 tok/s;   6576 sec\n",
            "[2020-09-20 17:57:30,973 INFO] Step 34500/200000; acc:  68.95; ppl:  3.54; xent: 1.26; lr: 0.00048; 6714/6517 tok/s;   6631 sec\n",
            "[2020-09-20 17:58:26,320 INFO] Step 34550/200000; acc:  69.12; ppl:  3.54; xent: 1.26; lr: 0.00048; 6494/6585 tok/s;   6686 sec\n",
            "[2020-09-20 17:59:21,013 INFO] Step 34600/200000; acc:  69.08; ppl:  3.53; xent: 1.26; lr: 0.00048; 6581/6626 tok/s;   6741 sec\n",
            "[2020-09-20 18:00:15,685 INFO] Step 34650/200000; acc:  68.97; ppl:  3.53; xent: 1.26; lr: 0.00047; 6701/6492 tok/s;   6795 sec\n",
            "[2020-09-20 18:01:10,958 INFO] Step 34700/200000; acc:  69.50; ppl:  3.45; xent: 1.24; lr: 0.00047; 6634/6562 tok/s;   6851 sec\n",
            "[2020-09-20 18:02:06,088 INFO] Step 34750/200000; acc:  69.58; ppl:  3.43; xent: 1.23; lr: 0.00047; 6589/6545 tok/s;   6906 sec\n",
            "[2020-09-20 18:03:01,828 INFO] Step 34800/200000; acc:  69.69; ppl:  3.40; xent: 1.22; lr: 0.00047; 6478/6511 tok/s;   6961 sec\n",
            "[2020-09-20 18:03:40,016 INFO] Loading dataset from data/multi-europarl.train.4.pt\n",
            "[2020-09-20 18:04:02,154 INFO] number of examples: 424031\n",
            "[2020-09-20 18:04:24,006 INFO] Step 34850/200000; acc:  69.00; ppl:  3.59; xent: 1.28; lr: 0.00047; 4412/4406 tok/s;   7044 sec\n",
            "[2020-09-20 18:05:19,260 INFO] Step 34900/200000; acc:  66.55; ppl:  4.16; xent: 1.43; lr: 0.00047; 6563/6668 tok/s;   7099 sec\n",
            "[2020-09-20 18:06:13,839 INFO] Step 34950/200000; acc:  66.82; ppl:  4.11; xent: 1.41; lr: 0.00047; 6651/6589 tok/s;   7153 sec\n",
            "[2020-09-20 18:07:08,390 INFO] Step 35000/200000; acc:  66.76; ppl:  4.11; xent: 1.41; lr: 0.00047; 6701/6549 tok/s;   7208 sec\n",
            "[2020-09-20 18:08:04,414 INFO] Step 35050/200000; acc:  67.08; ppl:  4.03; xent: 1.39; lr: 0.00047; 6491/6628 tok/s;   7264 sec\n",
            "[2020-09-20 18:08:58,818 INFO] Step 35100/200000; acc:  67.59; ppl:  3.93; xent: 1.37; lr: 0.00047; 6568/6507 tok/s;   7318 sec\n",
            "[2020-09-20 18:09:53,315 INFO] Step 35150/200000; acc:  67.08; ppl:  4.02; xent: 1.39; lr: 0.00047; 6681/6495 tok/s;   7373 sec\n",
            "[2020-09-20 18:10:48,850 INFO] Step 35200/200000; acc:  67.68; ppl:  3.86; xent: 1.35; lr: 0.00047; 6457/6601 tok/s;   7428 sec\n",
            "[2020-09-20 18:10:49,556 INFO] Saving checkpoint models/multi-europarl_step_35200.pt\n",
            "[2020-09-20 18:12:17,759 INFO] Step 35250/200000; acc:  67.32; ppl:  3.95; xent: 1.37; lr: 0.00047; 4096/3999 tok/s;   7517 sec\n",
            "[2020-09-20 18:13:13,219 INFO] Step 35300/200000; acc:  67.46; ppl:  3.90; xent: 1.36; lr: 0.00047; 6599/6546 tok/s;   7573 sec\n",
            "[2020-09-20 18:14:08,054 INFO] Step 35350/200000; acc:  68.02; ppl:  3.80; xent: 1.33; lr: 0.00047; 6607/6580 tok/s;   7628 sec\n",
            "[2020-09-20 18:15:03,631 INFO] Step 35400/200000; acc:  68.26; ppl:  3.74; xent: 1.32; lr: 0.00047; 6510/6654 tok/s;   7683 sec\n",
            "[2020-09-20 18:15:59,607 INFO] Step 35450/200000; acc:  67.64; ppl:  3.86; xent: 1.35; lr: 0.00047; 6406/6466 tok/s;   7739 sec\n",
            "[2020-09-20 18:16:55,018 INFO] Step 35500/200000; acc:  68.76; ppl:  3.64; xent: 1.29; lr: 0.00047; 6465/6570 tok/s;   7795 sec\n",
            "[2020-09-20 18:17:50,142 INFO] Step 35550/200000; acc:  68.36; ppl:  3.70; xent: 1.31; lr: 0.00047; 6509/6629 tok/s;   7850 sec\n",
            "[2020-09-20 18:18:46,301 INFO] Step 35600/200000; acc:  68.77; ppl:  3.64; xent: 1.29; lr: 0.00047; 6404/6660 tok/s;   7906 sec\n",
            "[2020-09-20 18:19:41,697 INFO] Step 35650/200000; acc:  68.04; ppl:  3.75; xent: 1.32; lr: 0.00047; 6493/6437 tok/s;   7961 sec\n",
            "[2020-09-20 18:20:36,743 INFO] Step 35700/200000; acc:  68.78; ppl:  3.61; xent: 1.28; lr: 0.00047; 6551/6577 tok/s;   8016 sec\n",
            "[2020-09-20 18:21:31,636 INFO] Step 35750/200000; acc:  69.04; ppl:  3.57; xent: 1.27; lr: 0.00047; 6556/6559 tok/s;   8071 sec\n",
            "[2020-09-20 18:22:26,964 INFO] Step 35800/200000; acc:  69.14; ppl:  3.56; xent: 1.27; lr: 0.00047; 6585/6551 tok/s;   8127 sec\n",
            "[2020-09-20 18:23:21,872 INFO] Step 35850/200000; acc:  69.12; ppl:  3.55; xent: 1.27; lr: 0.00047; 6595/6592 tok/s;   8181 sec\n",
            "[2020-09-20 18:24:16,551 INFO] Step 35900/200000; acc:  69.31; ppl:  3.50; xent: 1.25; lr: 0.00047; 6629/6597 tok/s;   8236 sec\n",
            "[2020-09-20 18:25:11,678 INFO] Step 35950/200000; acc:  69.04; ppl:  3.55; xent: 1.27; lr: 0.00047; 6539/6519 tok/s;   8291 sec\n",
            "[2020-09-20 18:26:08,071 INFO] Step 36000/200000; acc:  69.13; ppl:  3.51; xent: 1.25; lr: 0.00047; 6462/6608 tok/s;   8348 sec\n",
            "[2020-09-20 18:27:03,880 INFO] Step 36050/200000; acc:  69.61; ppl:  3.44; xent: 1.24; lr: 0.00047; 6475/6568 tok/s;   8403 sec\n",
            "[2020-09-20 18:27:58,135 INFO] Step 36100/200000; acc:  69.63; ppl:  3.44; xent: 1.24; lr: 0.00047; 6644/6374 tok/s;   8458 sec\n",
            "[2020-09-20 18:28:52,355 INFO] Step 36150/200000; acc:  69.69; ppl:  3.41; xent: 1.23; lr: 0.00046; 6639/6536 tok/s;   8512 sec\n",
            "[2020-09-20 18:29:48,264 INFO] Step 36200/200000; acc:  69.63; ppl:  3.42; xent: 1.23; lr: 0.00046; 6414/6484 tok/s;   8568 sec\n",
            "[2020-09-20 18:30:43,075 INFO] Step 36250/200000; acc:  70.05; ppl:  3.35; xent: 1.21; lr: 0.00046; 6491/6608 tok/s;   8623 sec\n",
            "[2020-09-20 18:31:37,926 INFO] Step 36300/200000; acc:  69.74; ppl:  3.39; xent: 1.22; lr: 0.00046; 6600/6588 tok/s;   8678 sec\n",
            "[2020-09-20 18:32:10,733 INFO] Loading dataset from data/multi-europarl.train.5.pt\n",
            "[2020-09-20 18:32:37,946 INFO] number of examples: 426172\n",
            "[2020-09-20 18:33:04,464 INFO] Step 36350/200000; acc:  68.79; ppl:  3.63; xent: 1.29; lr: 0.00046; 4094/4095 tok/s;   8764 sec\n",
            "[2020-09-20 18:33:59,946 INFO] Step 36400/200000; acc:  66.90; ppl:  4.07; xent: 1.40; lr: 0.00046; 6497/6638 tok/s;   8820 sec\n",
            "[2020-09-20 18:34:54,667 INFO] Step 36450/200000; acc:  67.23; ppl:  3.99; xent: 1.38; lr: 0.00046; 6713/6591 tok/s;   8874 sec\n",
            "[2020-09-20 18:35:50,375 INFO] Step 36500/200000; acc:  67.58; ppl:  3.92; xent: 1.37; lr: 0.00046; 6465/6650 tok/s;   8930 sec\n",
            "[2020-09-20 18:36:45,776 INFO] Step 36550/200000; acc:  67.44; ppl:  3.93; xent: 1.37; lr: 0.00046; 6551/6587 tok/s;   8985 sec\n",
            "[2020-09-20 18:37:40,883 INFO] Step 36600/200000; acc:  67.18; ppl:  3.96; xent: 1.38; lr: 0.00046; 6618/6525 tok/s;   9040 sec\n",
            "[2020-09-20 18:38:36,464 INFO] Step 36650/200000; acc:  67.91; ppl:  3.83; xent: 1.34; lr: 0.00046; 6566/6559 tok/s;   9096 sec\n",
            "[2020-09-20 18:39:31,898 INFO] Step 36700/200000; acc:  67.78; ppl:  3.84; xent: 1.34; lr: 0.00046; 6449/6544 tok/s;   9151 sec\n",
            "[2020-09-20 18:40:27,267 INFO] Step 36750/200000; acc:  67.81; ppl:  3.82; xent: 1.34; lr: 0.00046; 6620/6574 tok/s;   9207 sec\n",
            "[2020-09-20 18:41:22,902 INFO] Step 36800/200000; acc:  68.25; ppl:  3.75; xent: 1.32; lr: 0.00046; 6451/6664 tok/s;   9262 sec\n",
            "[2020-09-20 18:41:23,641 INFO] Saving checkpoint models/multi-europarl_step_36800.pt\n",
            "[2020-09-20 18:42:52,134 INFO] Step 36850/200000; acc:  67.72; ppl:  3.82; xent: 1.34; lr: 0.00046; 3979/3976 tok/s;   9352 sec\n",
            "[2020-09-20 18:43:47,821 INFO] Step 36900/200000; acc:  68.35; ppl:  3.71; xent: 1.31; lr: 0.00046; 6454/6532 tok/s;   9407 sec\n",
            "[2020-09-20 18:44:42,670 INFO] Step 36950/200000; acc:  68.24; ppl:  3.73; xent: 1.32; lr: 0.00046; 6472/6551 tok/s;   9462 sec\n",
            "[2020-09-20 18:45:38,420 INFO] Step 37000/200000; acc:  68.49; ppl:  3.68; xent: 1.30; lr: 0.00046; 6414/6579 tok/s;   9518 sec\n",
            "[2020-09-20 18:46:33,883 INFO] Step 37050/200000; acc:  68.63; ppl:  3.64; xent: 1.29; lr: 0.00046; 6414/6487 tok/s;   9573 sec\n",
            "[2020-09-20 18:47:28,739 INFO] Step 37100/200000; acc:  68.85; ppl:  3.59; xent: 1.28; lr: 0.00046; 6605/6515 tok/s;   9628 sec\n",
            "[2020-09-20 18:48:24,053 INFO] Step 37150/200000; acc:  69.34; ppl:  3.50; xent: 1.25; lr: 0.00046; 6496/6622 tok/s;   9684 sec\n",
            "[2020-09-20 18:49:19,711 INFO] Step 37200/200000; acc:  69.13; ppl:  3.55; xent: 1.27; lr: 0.00046; 6497/6517 tok/s;   9739 sec\n",
            "[2020-09-20 18:50:13,937 INFO] Step 37250/200000; acc:  68.80; ppl:  3.58; xent: 1.28; lr: 0.00046; 6731/6501 tok/s;   9794 sec\n",
            "[2020-09-20 18:51:09,664 INFO] Step 37300/200000; acc:  69.18; ppl:  3.52; xent: 1.26; lr: 0.00046; 6505/6578 tok/s;   9849 sec\n",
            "[2020-09-20 18:52:05,047 INFO] Step 37350/200000; acc:  69.39; ppl:  3.48; xent: 1.25; lr: 0.00046; 6540/6398 tok/s;   9905 sec\n",
            "[2020-09-20 18:53:00,064 INFO] Step 37400/200000; acc:  69.09; ppl:  3.50; xent: 1.25; lr: 0.00046; 6580/6535 tok/s;   9960 sec\n",
            "[2020-09-20 18:53:55,579 INFO] Step 37450/200000; acc:  69.42; ppl:  3.46; xent: 1.24; lr: 0.00046; 6595/6575 tok/s;  10015 sec\n",
            "[2020-09-20 18:54:49,909 INFO] Step 37500/200000; acc:  69.94; ppl:  3.39; xent: 1.22; lr: 0.00046; 6745/6591 tok/s;  10069 sec\n",
            "[2020-09-20 18:55:44,898 INFO] Step 37550/200000; acc:  69.87; ppl:  3.38; xent: 1.22; lr: 0.00046; 6592/6521 tok/s;  10124 sec\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TCxOa21QTMfb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}