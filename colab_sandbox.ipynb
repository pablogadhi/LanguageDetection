{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.2-final"
    },
    "orig_nbformat": 2,
    "kernelspec": {
      "name": "python38264bitlanguagedetectionpipenv382f6b637c4e47f9bb779a88a8777172",
      "display_name": "Python 3.8.2 64-bit ('LanguageDetection': pipenv)"
    },
    "colab": {
      "name": "colab_sandbox.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pablogadhi/LanguageDetection/blob/master/colab_sandbox.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kn1jfXR_ugxS",
        "colab_type": "text"
      },
      "source": [
        "#Environment Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q0b-cXd1uSBm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 364
        },
        "outputId": "7077dea2-c8ae-4b80-f920-79e335fa91d4"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sun Jul 19 22:42:13 2020       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 450.51.05    Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla P100-PCIE...  Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   36C    P0    26W / 250W |      0MiB / 16280MiB |      0%      Default |\n",
            "|                               |                      |                 ERR! |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vdAOlqqQuNcY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "836f53b9-6798-4ed2-d3e6-edbe8c45bdff"
      },
      "source": [
        "!git clone https://github.com/pablogadhi/LanguageDetection.git"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fatal: destination path 'LanguageDetection' already exists and is not an empty directory.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-x6l-fpMuNcj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 712
        },
        "outputId": "93fddb1c-fcbd-4520-e3ef-59304b4abbcf"
      },
      "source": [
        "!pip install OpenNMT-py"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: OpenNMT-py in /usr/local/lib/python3.6/dist-packages (1.1.1)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from OpenNMT-py) (0.16.0)\n",
            "Requirement already satisfied: torchtext==0.4.0 in /usr/local/lib/python3.6/dist-packages (from OpenNMT-py) (0.4.0)\n",
            "Requirement already satisfied: pyonmttok==1.*; platform_system == \"Linux\" in /usr/local/lib/python3.6/dist-packages (from OpenNMT-py) (1.18.5)\n",
            "Requirement already satisfied: tqdm~=4.30.0 in /usr/local/lib/python3.6/dist-packages (from OpenNMT-py) (4.30.0)\n",
            "Requirement already satisfied: torch>=1.4.0 in /usr/local/lib/python3.6/dist-packages (from OpenNMT-py) (1.5.1+cu101)\n",
            "Requirement already satisfied: waitress in /usr/local/lib/python3.6/dist-packages (from OpenNMT-py) (1.4.4)\n",
            "Requirement already satisfied: configargparse in /usr/local/lib/python3.6/dist-packages (from OpenNMT-py) (1.2.3)\n",
            "Requirement already satisfied: flask in /usr/local/lib/python3.6/dist-packages (from OpenNMT-py) (1.1.2)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from OpenNMT-py) (3.13)\n",
            "Requirement already satisfied: tensorboard>=1.14 in /usr/local/lib/python3.6/dist-packages (from OpenNMT-py) (2.2.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from OpenNMT-py) (1.12.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from torchtext==0.4.0->OpenNMT-py) (2.23.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torchtext==0.4.0->OpenNMT-py) (1.18.5)\n",
            "Requirement already satisfied: click>=5.1 in /usr/local/lib/python3.6/dist-packages (from flask->OpenNMT-py) (7.1.2)\n",
            "Requirement already satisfied: Jinja2>=2.10.1 in /usr/local/lib/python3.6/dist-packages (from flask->OpenNMT-py) (2.11.2)\n",
            "Requirement already satisfied: itsdangerous>=0.24 in /usr/local/lib/python3.6/dist-packages (from flask->OpenNMT-py) (1.1.0)\n",
            "Requirement already satisfied: Werkzeug>=0.15 in /usr/local/lib/python3.6/dist-packages (from flask->OpenNMT-py) (1.0.1)\n",
            "Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=1.14->OpenNMT-py) (3.12.2)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=1.14->OpenNMT-py) (0.4.1)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=1.14->OpenNMT-py) (1.17.2)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=1.14->OpenNMT-py) (49.1.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=1.14->OpenNMT-py) (1.7.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=1.14->OpenNMT-py) (3.2.2)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=1.14->OpenNMT-py) (0.9.0)\n",
            "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tensorboard>=1.14->OpenNMT-py) (0.34.2)\n",
            "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=1.14->OpenNMT-py) (1.30.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext==0.4.0->OpenNMT-py) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext==0.4.0->OpenNMT-py) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext==0.4.0->OpenNMT-py) (2020.6.20)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->torchtext==0.4.0->OpenNMT-py) (2.10)\n",
            "Requirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from Jinja2>=2.10.1->flask->OpenNMT-py) (1.1.1)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=1.14->OpenNMT-py) (1.3.0)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=1.14->OpenNMT-py) (4.1.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=1.14->OpenNMT-py) (4.6)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=1.14->OpenNMT-py) (0.2.8)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard>=1.14->OpenNMT-py) (1.7.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=1.14->OpenNMT-py) (3.1.0)\n",
            "Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.6/dist-packages (from rsa<5,>=3.1.4; python_version >= \"3\"->google-auth<2,>=1.6.3->tensorboard>=1.14->OpenNMT-py) (0.4.8)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard>=1.14->OpenNMT-py) (3.1.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_I-Sa3hWucdx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 141
        },
        "outputId": "95e40d65-6ab6-47e3-8912-ba7814356175"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "%cd drive/My\\ Drive/Tesis"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n",
            "/content/drive/My Drive/Tesis\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2tQbz5Yhu1SY",
        "colab_type": "text"
      },
      "source": [
        "#Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CkOtW8aIvGzv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "30258919-64da-45d2-e2dd-5ddf50296921"
      },
      "source": [
        "!../../../LanguageDetection/onmt_train.sh data/multi-europarl models/multi-europarl 100000 100001 4096 2500 models/multi-europarl_step_50000.pt"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[2020-07-19 21:54:42,401 INFO] Loading checkpoint from models/multi-europarl_step_50000.pt\n",
            "[2020-07-19 21:54:44,001 INFO] Loading vocab from checkpoint at models/multi-europarl_step_50000.pt.\n",
            "[2020-07-19 21:54:44,001 INFO]  * src vocab size = 32288\n",
            "[2020-07-19 21:54:44,001 INFO]  * tgt vocab size = 32280\n",
            "[2020-07-19 21:54:44,001 INFO] Building model...\n",
            "[2020-07-19 21:54:54,534 INFO] NMTModel(\n",
            "  (encoder): TransformerEncoder(\n",
            "    (embeddings): Embeddings(\n",
            "      (make_embedding): Sequential(\n",
            "        (emb_luts): Elementwise(\n",
            "          (0): Embedding(32288, 512, padding_idx=1)\n",
            "        )\n",
            "        (pe): PositionalEncoding(\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (transformer): ModuleList(\n",
            "      (0): TransformerEncoderLayer(\n",
            "        (self_attn): MultiHeadedAttention(\n",
            "          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (softmax): Softmax(dim=-1)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (feed_forward): PositionwiseFeedForward(\n",
            "          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
            "          (dropout_1): Dropout(p=0.1, inplace=False)\n",
            "          (relu): ReLU()\n",
            "          (dropout_2): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "      (1): TransformerEncoderLayer(\n",
            "        (self_attn): MultiHeadedAttention(\n",
            "          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (softmax): Softmax(dim=-1)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (feed_forward): PositionwiseFeedForward(\n",
            "          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
            "          (dropout_1): Dropout(p=0.1, inplace=False)\n",
            "          (relu): ReLU()\n",
            "          (dropout_2): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "      (2): TransformerEncoderLayer(\n",
            "        (self_attn): MultiHeadedAttention(\n",
            "          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (softmax): Softmax(dim=-1)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (feed_forward): PositionwiseFeedForward(\n",
            "          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
            "          (dropout_1): Dropout(p=0.1, inplace=False)\n",
            "          (relu): ReLU()\n",
            "          (dropout_2): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "      (3): TransformerEncoderLayer(\n",
            "        (self_attn): MultiHeadedAttention(\n",
            "          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (softmax): Softmax(dim=-1)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (feed_forward): PositionwiseFeedForward(\n",
            "          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
            "          (dropout_1): Dropout(p=0.1, inplace=False)\n",
            "          (relu): ReLU()\n",
            "          (dropout_2): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "      (4): TransformerEncoderLayer(\n",
            "        (self_attn): MultiHeadedAttention(\n",
            "          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (softmax): Softmax(dim=-1)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (feed_forward): PositionwiseFeedForward(\n",
            "          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
            "          (dropout_1): Dropout(p=0.1, inplace=False)\n",
            "          (relu): ReLU()\n",
            "          (dropout_2): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "      (5): TransformerEncoderLayer(\n",
            "        (self_attn): MultiHeadedAttention(\n",
            "          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (softmax): Softmax(dim=-1)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (feed_forward): PositionwiseFeedForward(\n",
            "          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
            "          (dropout_1): Dropout(p=0.1, inplace=False)\n",
            "          (relu): ReLU()\n",
            "          (dropout_2): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "    )\n",
            "    (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
            "  )\n",
            "  (decoder): TransformerDecoder(\n",
            "    (embeddings): Embeddings(\n",
            "      (make_embedding): Sequential(\n",
            "        (emb_luts): Elementwise(\n",
            "          (0): Embedding(32280, 512, padding_idx=1)\n",
            "        )\n",
            "        (pe): PositionalEncoding(\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (transformer_layers): ModuleList(\n",
            "      (0): TransformerDecoderLayer(\n",
            "        (self_attn): MultiHeadedAttention(\n",
            "          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (softmax): Softmax(dim=-1)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (context_attn): MultiHeadedAttention(\n",
            "          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (softmax): Softmax(dim=-1)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (feed_forward): PositionwiseFeedForward(\n",
            "          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
            "          (dropout_1): Dropout(p=0.1, inplace=False)\n",
            "          (relu): ReLU()\n",
            "          (dropout_2): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (layer_norm_1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
            "        (layer_norm_2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
            "        (drop): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "      (1): TransformerDecoderLayer(\n",
            "        (self_attn): MultiHeadedAttention(\n",
            "          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (softmax): Softmax(dim=-1)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (context_attn): MultiHeadedAttention(\n",
            "          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (softmax): Softmax(dim=-1)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (feed_forward): PositionwiseFeedForward(\n",
            "          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
            "          (dropout_1): Dropout(p=0.1, inplace=False)\n",
            "          (relu): ReLU()\n",
            "          (dropout_2): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (layer_norm_1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
            "        (layer_norm_2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
            "        (drop): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "      (2): TransformerDecoderLayer(\n",
            "        (self_attn): MultiHeadedAttention(\n",
            "          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (softmax): Softmax(dim=-1)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (context_attn): MultiHeadedAttention(\n",
            "          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (softmax): Softmax(dim=-1)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (feed_forward): PositionwiseFeedForward(\n",
            "          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
            "          (dropout_1): Dropout(p=0.1, inplace=False)\n",
            "          (relu): ReLU()\n",
            "          (dropout_2): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (layer_norm_1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
            "        (layer_norm_2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
            "        (drop): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "      (3): TransformerDecoderLayer(\n",
            "        (self_attn): MultiHeadedAttention(\n",
            "          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (softmax): Softmax(dim=-1)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (context_attn): MultiHeadedAttention(\n",
            "          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (softmax): Softmax(dim=-1)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (feed_forward): PositionwiseFeedForward(\n",
            "          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
            "          (dropout_1): Dropout(p=0.1, inplace=False)\n",
            "          (relu): ReLU()\n",
            "          (dropout_2): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (layer_norm_1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
            "        (layer_norm_2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
            "        (drop): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "      (4): TransformerDecoderLayer(\n",
            "        (self_attn): MultiHeadedAttention(\n",
            "          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (softmax): Softmax(dim=-1)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (context_attn): MultiHeadedAttention(\n",
            "          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (softmax): Softmax(dim=-1)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (feed_forward): PositionwiseFeedForward(\n",
            "          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
            "          (dropout_1): Dropout(p=0.1, inplace=False)\n",
            "          (relu): ReLU()\n",
            "          (dropout_2): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (layer_norm_1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
            "        (layer_norm_2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
            "        (drop): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "      (5): TransformerDecoderLayer(\n",
            "        (self_attn): MultiHeadedAttention(\n",
            "          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (softmax): Softmax(dim=-1)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (context_attn): MultiHeadedAttention(\n",
            "          (linear_keys): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (linear_values): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (linear_query): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (softmax): Softmax(dim=-1)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "          (final_linear): Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "        (feed_forward): PositionwiseFeedForward(\n",
            "          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "          (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
            "          (dropout_1): Dropout(p=0.1, inplace=False)\n",
            "          (relu): ReLU()\n",
            "          (dropout_2): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (layer_norm_1): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
            "        (layer_norm_2): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
            "        (drop): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "    )\n",
            "    (layer_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)\n",
            "  )\n",
            "  (generator): Sequential(\n",
            "    (0): Linear(in_features=512, out_features=32280, bias=True)\n",
            "    (1): Cast()\n",
            "    (2): LogSoftmax()\n",
            "  )\n",
            ")\n",
            "[2020-07-19 21:54:54,620 INFO] encoder: 35446784\n",
            "[2020-07-19 21:54:54,620 INFO] decoder: 58312216\n",
            "[2020-07-19 21:54:54,620 INFO] * number of parameters: 93759000\n",
            "[2020-07-19 21:54:55,245 INFO] Starting training on GPU: [0]\n",
            "[2020-07-19 21:54:55,245 INFO] Start training loop and validate every 100001 steps...\n",
            "[2020-07-19 21:54:55,246 INFO] Loading dataset from data/multi-europarl.train.0.pt\n",
            "[2020-07-19 21:55:15,834 INFO] number of examples: 816825\n",
            "[2020-07-19 21:55:56,283 INFO] Step 50050/100000; acc:  69.91; ppl:  3.45; xent: 1.24; lr: 0.00040; 5987/6091 tok/s;     61 sec\n",
            "[2020-07-19 21:56:30,092 INFO] Step 50100/100000; acc:  69.85; ppl:  3.45; xent: 1.24; lr: 0.00039; 10683/10853 tok/s;     95 sec\n",
            "[2020-07-19 21:57:03,934 INFO] Step 50150/100000; acc:  69.85; ppl:  3.46; xent: 1.24; lr: 0.00039; 10510/10815 tok/s;    129 sec\n",
            "[2020-07-19 21:57:37,297 INFO] Step 50200/100000; acc:  70.68; ppl:  3.31; xent: 1.20; lr: 0.00039; 11083/10744 tok/s;    162 sec\n",
            "[2020-07-19 21:58:10,722 INFO] Step 50250/100000; acc:  70.29; ppl:  3.35; xent: 1.21; lr: 0.00039; 10745/10844 tok/s;    195 sec\n",
            "[2020-07-19 21:58:44,504 INFO] Step 50300/100000; acc:  70.11; ppl:  3.40; xent: 1.22; lr: 0.00039; 10699/10794 tok/s;    229 sec\n",
            "[2020-07-19 21:59:18,696 INFO] Step 50350/100000; acc:  70.29; ppl:  3.37; xent: 1.22; lr: 0.00039; 10588/10699 tok/s;    263 sec\n",
            "[2020-07-19 21:59:52,440 INFO] Step 50400/100000; acc:  70.80; ppl:  3.29; xent: 1.19; lr: 0.00039; 10653/10796 tok/s;    297 sec\n",
            "[2020-07-19 22:00:25,921 INFO] Step 50450/100000; acc:  71.03; ppl:  3.27; xent: 1.18; lr: 0.00039; 10757/10960 tok/s;    331 sec\n",
            "[2020-07-19 22:01:00,024 INFO] Step 50500/100000; acc:  70.37; ppl:  3.35; xent: 1.21; lr: 0.00039; 10579/10904 tok/s;    365 sec\n",
            "[2020-07-19 22:01:33,645 INFO] Step 50550/100000; acc:  70.73; ppl:  3.31; xent: 1.20; lr: 0.00039; 10864/10648 tok/s;    398 sec\n",
            "[2020-07-19 22:02:07,041 INFO] Step 50600/100000; acc:  70.71; ppl:  3.30; xent: 1.19; lr: 0.00039; 10858/10832 tok/s;    432 sec\n",
            "[2020-07-19 22:02:40,588 INFO] Step 50650/100000; acc:  70.68; ppl:  3.31; xent: 1.20; lr: 0.00039; 10813/10782 tok/s;    465 sec\n",
            "[2020-07-19 22:03:14,540 INFO] Step 50700/100000; acc:  70.55; ppl:  3.32; xent: 1.20; lr: 0.00039; 10938/10705 tok/s;    499 sec\n",
            "[2020-07-19 22:03:47,829 INFO] Step 50750/100000; acc:  70.84; ppl:  3.28; xent: 1.19; lr: 0.00039; 10882/10746 tok/s;    533 sec\n",
            "[2020-07-19 22:04:21,277 INFO] Step 50800/100000; acc:  71.23; ppl:  3.22; xent: 1.17; lr: 0.00039; 11023/10877 tok/s;    566 sec\n",
            "[2020-07-19 22:04:54,915 INFO] Step 50850/100000; acc:  70.85; ppl:  3.28; xent: 1.19; lr: 0.00039; 10746/10911 tok/s;    600 sec\n",
            "[2020-07-19 22:05:28,672 INFO] Step 50900/100000; acc:  70.68; ppl:  3.30; xent: 1.19; lr: 0.00039; 10870/10713 tok/s;    633 sec\n",
            "[2020-07-19 22:06:02,308 INFO] Step 50950/100000; acc:  71.39; ppl:  3.20; xent: 1.16; lr: 0.00039; 10807/10899 tok/s;    667 sec\n",
            "[2020-07-19 22:06:35,864 INFO] Step 51000/100000; acc:  71.39; ppl:  3.20; xent: 1.16; lr: 0.00039; 10710/10870 tok/s;    701 sec\n",
            "[2020-07-19 22:07:09,470 INFO] Step 51050/100000; acc:  70.81; ppl:  3.28; xent: 1.19; lr: 0.00039; 10776/10888 tok/s;    734 sec\n",
            "[2020-07-19 22:07:42,948 INFO] Step 51100/100000; acc:  71.62; ppl:  3.16; xent: 1.15; lr: 0.00039; 10834/10840 tok/s;    768 sec\n",
            "[2020-07-19 22:08:17,176 INFO] Step 51150/100000; acc:  71.06; ppl:  3.23; xent: 1.17; lr: 0.00039; 10553/10978 tok/s;    802 sec\n",
            "[2020-07-19 22:08:50,924 INFO] Step 51200/100000; acc:  70.97; ppl:  3.26; xent: 1.18; lr: 0.00039; 10660/10551 tok/s;    836 sec\n",
            "[2020-07-19 22:09:24,243 INFO] Step 51250/100000; acc:  71.71; ppl:  3.14; xent: 1.14; lr: 0.00039; 10760/10901 tok/s;    869 sec\n",
            "[2020-07-19 22:09:57,858 INFO] Step 51300/100000; acc:  71.39; ppl:  3.19; xent: 1.16; lr: 0.00039; 10925/10871 tok/s;    903 sec\n",
            "[2020-07-19 22:10:31,725 INFO] Step 51350/100000; acc:  71.57; ppl:  3.17; xent: 1.15; lr: 0.00039; 10875/10894 tok/s;    936 sec\n",
            "[2020-07-19 22:11:05,453 INFO] Step 51400/100000; acc:  71.66; ppl:  3.16; xent: 1.15; lr: 0.00039; 10833/10901 tok/s;    970 sec\n",
            "[2020-07-19 22:11:39,671 INFO] Step 51450/100000; acc:  71.91; ppl:  3.12; xent: 1.14; lr: 0.00039; 10814/10866 tok/s;   1004 sec\n",
            "[2020-07-19 22:12:13,068 INFO] Step 51500/100000; acc:  71.52; ppl:  3.17; xent: 1.15; lr: 0.00039; 10836/10803 tok/s;   1038 sec\n",
            "[2020-07-19 22:12:46,257 INFO] Step 51550/100000; acc:  71.20; ppl:  3.22; xent: 1.17; lr: 0.00039; 10879/10635 tok/s;   1071 sec\n",
            "[2020-07-19 22:13:19,878 INFO] Step 51600/100000; acc:  71.60; ppl:  3.16; xent: 1.15; lr: 0.00039; 10889/10833 tok/s;   1105 sec\n",
            "[2020-07-19 22:13:53,674 INFO] Step 51650/100000; acc:  71.39; ppl:  3.19; xent: 1.16; lr: 0.00039; 10741/10780 tok/s;   1138 sec\n",
            "[2020-07-19 22:14:26,727 INFO] Step 51700/100000; acc:  71.43; ppl:  3.19; xent: 1.16; lr: 0.00039; 10910/10637 tok/s;   1171 sec\n",
            "[2020-07-19 22:15:00,385 INFO] Step 51750/100000; acc:  71.42; ppl:  3.18; xent: 1.16; lr: 0.00039; 10846/10842 tok/s;   1205 sec\n",
            "[2020-07-19 22:15:33,770 INFO] Step 51800/100000; acc:  71.22; ppl:  3.20; xent: 1.16; lr: 0.00039; 11007/10725 tok/s;   1239 sec\n",
            "[2020-07-19 22:16:07,519 INFO] Step 51850/100000; acc:  71.37; ppl:  3.18; xent: 1.16; lr: 0.00039; 10882/10822 tok/s;   1272 sec\n",
            "[2020-07-19 22:16:40,745 INFO] Step 51900/100000; acc:  71.66; ppl:  3.14; xent: 1.14; lr: 0.00039; 10824/10815 tok/s;   1305 sec\n",
            "[2020-07-19 22:17:13,933 INFO] Step 51950/100000; acc:  71.44; ppl:  3.19; xent: 1.16; lr: 0.00039; 10853/10741 tok/s;   1339 sec\n",
            "[2020-07-19 22:17:47,996 INFO] Step 52000/100000; acc:  71.74; ppl:  3.14; xent: 1.14; lr: 0.00039; 10592/10895 tok/s;   1373 sec\n",
            "[2020-07-19 22:18:21,766 INFO] Step 52050/100000; acc:  71.90; ppl:  3.12; xent: 1.14; lr: 0.00039; 10837/10797 tok/s;   1407 sec\n",
            "[2020-07-19 22:18:55,007 INFO] Step 52100/100000; acc:  71.81; ppl:  3.12; xent: 1.14; lr: 0.00039; 10802/10780 tok/s;   1440 sec\n",
            "[2020-07-19 22:19:28,819 INFO] Step 52150/100000; acc:  71.22; ppl:  3.21; xent: 1.17; lr: 0.00039; 10727/10733 tok/s;   1474 sec\n",
            "[2020-07-19 22:20:02,721 INFO] Step 52200/100000; acc:  71.19; ppl:  3.21; xent: 1.17; lr: 0.00039; 10734/10689 tok/s;   1507 sec\n",
            "[2020-07-19 22:20:36,421 INFO] Step 52250/100000; acc:  71.76; ppl:  3.12; xent: 1.14; lr: 0.00039; 10672/10898 tok/s;   1541 sec\n",
            "[2020-07-19 22:21:10,316 INFO] Step 52300/100000; acc:  71.72; ppl:  3.13; xent: 1.14; lr: 0.00039; 10807/10846 tok/s;   1575 sec\n",
            "[2020-07-19 22:21:45,068 INFO] Step 52350/100000; acc:  71.93; ppl:  3.11; xent: 1.14; lr: 0.00039; 10463/10371 tok/s;   1610 sec\n",
            "[2020-07-19 22:22:18,589 INFO] Step 52400/100000; acc:  71.47; ppl:  3.17; xent: 1.15; lr: 0.00039; 10298/10375 tok/s;   1643 sec\n",
            "[2020-07-19 22:22:53,241 INFO] Step 52450/100000; acc:  71.76; ppl:  3.12; xent: 1.14; lr: 0.00039; 10577/10785 tok/s;   1678 sec\n",
            "[2020-07-19 22:23:27,288 INFO] Step 52500/100000; acc:  71.99; ppl:  3.11; xent: 1.14; lr: 0.00039; 10885/10633 tok/s;   1712 sec\n",
            "[2020-07-19 22:23:27,471 INFO] Saving checkpoint models/multi-europarl_step_52500.pt\n",
            "[2020-07-19 22:24:06,011 INFO] Step 52550/100000; acc:  71.70; ppl:  3.14; xent: 1.15; lr: 0.00039; 9101/8996 tok/s;   1751 sec\n",
            "[2020-07-19 22:24:39,967 INFO] Step 52600/100000; acc:  71.57; ppl:  3.15; xent: 1.15; lr: 0.00039; 10638/10655 tok/s;   1785 sec\n",
            "[2020-07-19 22:25:13,345 INFO] Step 52650/100000; acc:  72.19; ppl:  3.07; xent: 1.12; lr: 0.00039; 10623/10558 tok/s;   1818 sec\n",
            "[2020-07-19 22:25:47,139 INFO] Step 52700/100000; acc:  71.97; ppl:  3.09; xent: 1.13; lr: 0.00039; 10407/10603 tok/s;   1852 sec\n",
            "[2020-07-19 22:26:20,890 INFO] Step 52750/100000; acc:  72.14; ppl:  3.07; xent: 1.12; lr: 0.00038; 10828/10724 tok/s;   1886 sec\n",
            "[2020-07-19 22:26:54,721 INFO] Step 52800/100000; acc:  71.94; ppl:  3.09; xent: 1.13; lr: 0.00038; 10544/10534 tok/s;   1919 sec\n",
            "[2020-07-19 22:27:28,467 INFO] Step 52850/100000; acc:  72.10; ppl:  3.08; xent: 1.13; lr: 0.00038; 10385/10704 tok/s;   1953 sec\n",
            "[2020-07-19 22:28:02,367 INFO] Step 52900/100000; acc:  72.48; ppl:  3.02; xent: 1.11; lr: 0.00038; 10789/10786 tok/s;   1987 sec\n",
            "[2020-07-19 22:28:36,298 INFO] Step 52950/100000; acc:  72.16; ppl:  3.07; xent: 1.12; lr: 0.00038; 10605/10554 tok/s;   2021 sec\n",
            "[2020-07-19 22:28:36,973 INFO] Loading dataset from data/multi-europarl.train.1.pt\n",
            "../../../LanguageDetection/onmt_train.sh: line 39:   490 Killed                  onmt_train -data $1 -save_model $2 -layers 6 -rnn_size 512 -word_vec_size 512 -transformer_ff 2048 -heads 8 -encoder_type transformer -decoder_type transformer -position_encoding -train_steps $steps -max_generator_batches 2 -dropout 0.1 -batch_size $batch_size -batch_type tokens -normalization tokens -accum_count 2 -optim adam -adam_beta2 0.998 -decay_method noam -warmup_steps 8000 -learning_rate 2 -max_grad_norm 0 -param_init 0 -param_init_glorot -label_smoothing 0.1 -valid_steps $val_steps -save_checkpoint_steps $save_at -world_size 1 -gpu_ranks 0 -train_from \"$checkpoint\"\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xp02G73uHhCd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3UgypO86JzhU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QjCdXUnSMGAT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fT1hL8YrOYfV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}